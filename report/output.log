/workspace/pytorch_project_template-main/.venv/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
HiFiGANGeneratorV2(
  (pre): Conv1d(80, 128, kernel_size=(7,), stride=(1,), padding=(3,))
  (ups): ModuleList(
    (0): ConvTranspose1d(128, 64, kernel_size=(16,), stride=(8,), padding=(4,))
    (1): ConvTranspose1d(64, 32, kernel_size=(16,), stride=(8,), padding=(4,))
    (2): ConvTranspose1d(32, 16, kernel_size=(4,), stride=(2,), padding=(1,))
    (3): ConvTranspose1d(16, 8, kernel_size=(4,), stride=(2,), padding=(1,))
  )
  (mrfs): ModuleList(
    (0): MRF(
      (blocks): ModuleList(
        (0): ResBlock1(
          (convs1): ModuleList(
            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
            (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
          )
          (convs2): ModuleList(
            (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
          )
        )
        (1): ResBlock1(
          (convs1): ModuleList(
            (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))
            (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
            (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
          )
          (convs2): ModuleList(
            (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))
          )
        )
        (2): ResBlock1(
          (convs1): ModuleList(
            (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))
            (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
            (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
          )
          (convs2): ModuleList(
            (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))
          )
        )
      )
    )
    (1): MRF(
      (blocks): ModuleList(
        (0): ResBlock1(
          (convs1): ModuleList(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
            (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
          )
          (convs2): ModuleList(
            (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
          )
        )
        (1): ResBlock1(
          (convs1): ModuleList(
            (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))
            (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
            (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
          )
          (convs2): ModuleList(
            (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))
          )
        )
        (2): ResBlock1(
          (convs1): ModuleList(
            (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))
            (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
            (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
          )
          (convs2): ModuleList(
            (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))
          )
        )
      )
    )
    (2): MRF(
      (blocks): ModuleList(
        (0): ResBlock1(
          (convs1): ModuleList(
            (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
            (2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
          )
          (convs2): ModuleList(
            (0-2): 3 x Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))
          )
        )
        (1): ResBlock1(
          (convs1): ModuleList(
            (0): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))
            (1): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
            (2): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
          )
          (convs2): ModuleList(
            (0-2): 3 x Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))
          )
        )
        (2): ResBlock1(
          (convs1): ModuleList(
            (0): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))
            (1): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
            (2): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
          )
          (convs2): ModuleList(
            (0-2): 3 x Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))
          )
        )
      )
    )
    (3): MRF(
      (blocks): ModuleList(
        (0): ResBlock1(
          (convs1): ModuleList(
            (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
            (2): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
          )
          (convs2): ModuleList(
            (0-2): 3 x Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,))
          )
        )
        (1): ResBlock1(
          (convs1): ModuleList(
            (0): Conv1d(8, 8, kernel_size=(7,), stride=(1,), padding=(3,))
            (1): Conv1d(8, 8, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
            (2): Conv1d(8, 8, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
          )
          (convs2): ModuleList(
            (0-2): 3 x Conv1d(8, 8, kernel_size=(7,), stride=(1,), padding=(3,))
          )
        )
        (2): ResBlock1(
          (convs1): ModuleList(
            (0): Conv1d(8, 8, kernel_size=(11,), stride=(1,), padding=(5,))
            (1): Conv1d(8, 8, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
            (2): Conv1d(8, 8, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
          )
          (convs2): ModuleList(
            (0-2): 3 x Conv1d(8, 8, kernel_size=(11,), stride=(1,), padding=(5,))
          )
        )
      )
    )
  )
  (post): Conv1d(8, 1, kernel_size=(7,), stride=(1,), padding=(3,))
)
Loading checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch70.pth
/workspace/pytorch_project_template-main/src/trainer/hifigan_trainer.py:130: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(resume_path, map_location=self.device)
Resume from epoch 71
train:   0%|â–Œ                                                                                                                                         | 5/1248 [00:04<11:07,  1.86it/s][34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 87360 that is less than the current step 87361. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Train Epoch: 71 [0/1248 (0%)] Loss: 30.993294
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:42<00:00,  3.64it/s]
Train Epoch: 71 [50/1248 (4%)] Loss: 28.953140
Train Epoch: 71 [100/1248 (8%)] Loss: 29.891760
Train Epoch: 71 [150/1248 (12%)] Loss: 30.871460
Train Epoch: 71 [200/1248 (16%)] Loss: 30.248020
Train Epoch: 71 [250/1248 (20%)] Loss: 30.917900
Train Epoch: 71 [300/1248 (24%)] Loss: 31.443314
Train Epoch: 71 [350/1248 (28%)] Loss: 31.152546
Train Epoch: 71 [400/1248 (32%)] Loss: 31.959227
Train Epoch: 71 [450/1248 (36%)] Loss: 30.535410
Train Epoch: 71 [500/1248 (40%)] Loss: 28.159674
Train Epoch: 71 [550/1248 (44%)] Loss: 29.097715
Train Epoch: 71 [600/1248 (48%)] Loss: 30.608986
Train Epoch: 71 [650/1248 (52%)] Loss: 30.945879
Train Epoch: 71 [700/1248 (56%)] Loss: 29.459080
Train Epoch: 71 [750/1248 (60%)] Loss: 29.616617
Train Epoch: 71 [800/1248 (64%)] Loss: 30.734661
Train Epoch: 71 [850/1248 (68%)] Loss: 28.088810
Train Epoch: 71 [900/1248 (72%)] Loss: 31.410633
Train Epoch: 71 [950/1248 (76%)] Loss: 27.686445
Train Epoch: 71 [1000/1248 (80%)] Loss: 30.794180
Train Epoch: 71 [1050/1248 (84%)] Loss: 32.117458
Train Epoch: 71 [1100/1248 (88%)] Loss: 31.189627
Train Epoch: 71 [1150/1248 (92%)] Loss: 30.890390
Train Epoch: 71 [1200/1248 (96%)] Loss: 31.276335
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.98it/s]
    epoch          : 71
    loss           : 31.060606918334962
    grad_norm      : 2500.5837927246093
    val_loss       : 28.427167261247156
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch71.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:25<00:00,  3.83it/s]
Train Epoch: 72 [0/1248 (0%)] Loss: 33.162601
Train Epoch: 72 [50/1248 (4%)] Loss: 30.842422
Train Epoch: 72 [100/1248 (8%)] Loss: 29.198711
Train Epoch: 72 [150/1248 (12%)] Loss: 32.158428
Train Epoch: 72 [200/1248 (16%)] Loss: 29.837622
Train Epoch: 72 [250/1248 (20%)] Loss: 29.014423
Train Epoch: 72 [300/1248 (24%)] Loss: 30.907400
Train Epoch: 72 [350/1248 (28%)] Loss: 32.058289
Train Epoch: 72 [400/1248 (32%)] Loss: 29.235790
Train Epoch: 72 [450/1248 (36%)] Loss: 33.391727
Train Epoch: 72 [500/1248 (40%)] Loss: 30.987629
Train Epoch: 72 [550/1248 (44%)] Loss: 29.847212
Train Epoch: 72 [600/1248 (48%)] Loss: 31.351337
Train Epoch: 72 [650/1248 (52%)] Loss: 32.193680
Train Epoch: 72 [700/1248 (56%)] Loss: 31.544329
Train Epoch: 72 [750/1248 (60%)] Loss: 29.639763
Train Epoch: 72 [800/1248 (64%)] Loss: 32.695339
Train Epoch: 72 [850/1248 (68%)] Loss: 27.973637
Train Epoch: 72 [900/1248 (72%)] Loss: 29.379740
Train Epoch: 72 [950/1248 (76%)] Loss: 26.526449
Train Epoch: 72 [1000/1248 (80%)] Loss: 27.642235
Train Epoch: 72 [1050/1248 (84%)] Loss: 29.541901
Train Epoch: 72 [1100/1248 (88%)] Loss: 33.675938
Train Epoch: 72 [1150/1248 (92%)] Loss: 33.257462
Train Epoch: 72 [1200/1248 (96%)] Loss: 33.010216
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.05it/s]
    epoch          : 72
    loss           : 31.363669815063478
    grad_norm      : 2617.5845343017577
    val_loss       : 26.92980041778345
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch72.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:20<00:00,  3.90it/s]
Train Epoch: 73 [0/1248 (0%)] Loss: 31.808228
Train Epoch: 73 [50/1248 (4%)] Loss: 31.881702
Train Epoch: 73 [100/1248 (8%)] Loss: 30.725534
Train Epoch: 73 [150/1248 (12%)] Loss: 30.831783
Train Epoch: 73 [200/1248 (16%)] Loss: 29.696152
Train Epoch: 73 [250/1248 (20%)] Loss: 31.799137
Train Epoch: 73 [300/1248 (24%)] Loss: 31.021015
Train Epoch: 73 [350/1248 (28%)] Loss: 30.312941
Train Epoch: 73 [400/1248 (32%)] Loss: 31.664276
Train Epoch: 73 [450/1248 (36%)] Loss: 29.582130
Train Epoch: 73 [500/1248 (40%)] Loss: 29.843307
Train Epoch: 73 [550/1248 (44%)] Loss: 26.401031
Train Epoch: 73 [600/1248 (48%)] Loss: 31.886585
Train Epoch: 73 [650/1248 (52%)] Loss: 31.109314
Train Epoch: 73 [700/1248 (56%)] Loss: 29.159695
Train Epoch: 73 [750/1248 (60%)] Loss: 29.903978
Train Epoch: 73 [800/1248 (64%)] Loss: 33.088898
Train Epoch: 73 [850/1248 (68%)] Loss: 31.858030
Train Epoch: 73 [900/1248 (72%)] Loss: 35.964558
Train Epoch: 73 [950/1248 (76%)] Loss: 32.481133
Train Epoch: 73 [1000/1248 (80%)] Loss: 29.344303
Train Epoch: 73 [1050/1248 (84%)] Loss: 28.554544
Train Epoch: 73 [1100/1248 (88%)] Loss: 30.801313
Train Epoch: 73 [1150/1248 (92%)] Loss: 31.057226
Train Epoch: 73 [1200/1248 (96%)] Loss: 31.272661
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  6.01it/s]
    epoch          : 73
    loss           : 30.902837028503416
    grad_norm      : 2895.058868408203
    val_loss       : 27.90940312172869
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch73.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:25<00:00,  3.83it/s]
Train Epoch: 74 [0/1248 (0%)] Loss: 31.924395
Train Epoch: 74 [50/1248 (4%)] Loss: 33.639080
Train Epoch: 74 [100/1248 (8%)] Loss: 29.088800
Train Epoch: 74 [150/1248 (12%)] Loss: 31.841288
Train Epoch: 74 [200/1248 (16%)] Loss: 31.133541
Train Epoch: 74 [250/1248 (20%)] Loss: 32.471752
Train Epoch: 74 [300/1248 (24%)] Loss: 31.210812
Train Epoch: 74 [350/1248 (28%)] Loss: 32.899014
Train Epoch: 74 [400/1248 (32%)] Loss: 32.516453
Train Epoch: 74 [450/1248 (36%)] Loss: 31.681377
Train Epoch: 74 [500/1248 (40%)] Loss: 31.728464
Train Epoch: 74 [550/1248 (44%)] Loss: 32.705162
Train Epoch: 74 [600/1248 (48%)] Loss: 30.358871
Train Epoch: 74 [650/1248 (52%)] Loss: 32.892605
Train Epoch: 74 [700/1248 (56%)] Loss: 32.025578
Train Epoch: 74 [750/1248 (60%)] Loss: 32.243713
Train Epoch: 74 [800/1248 (64%)] Loss: 29.954176
Train Epoch: 74 [850/1248 (68%)] Loss: 33.455734
Train Epoch: 74 [900/1248 (72%)] Loss: 31.306570
Train Epoch: 74 [950/1248 (76%)] Loss: 29.367096
Train Epoch: 74 [1000/1248 (80%)] Loss: 32.907700
Train Epoch: 74 [1050/1248 (84%)] Loss: 32.739193
Train Epoch: 74 [1100/1248 (88%)] Loss: 32.620670
Train Epoch: 74 [1150/1248 (92%)] Loss: 27.955339
Train Epoch: 74 [1200/1248 (96%)] Loss: 30.748165
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.06it/s]
    epoch          : 74
    loss           : 31.271114158630372
    grad_norm      : 3079.3972937011717
    val_loss       : 27.775379386737193
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch74.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:25<00:00,  3.83it/s]
Train Epoch: 75 [0/1248 (0%)] Loss: 29.929058
Train Epoch: 75 [50/1248 (4%)] Loss: 34.256863
Train Epoch: 75 [100/1248 (8%)] Loss: 31.820225
Train Epoch: 75 [150/1248 (12%)] Loss: 30.319757
Train Epoch: 75 [200/1248 (16%)] Loss: 30.827539
Train Epoch: 75 [250/1248 (20%)] Loss: 30.627487
Train Epoch: 75 [300/1248 (24%)] Loss: 31.409199
Train Epoch: 75 [350/1248 (28%)] Loss: 32.217197
Train Epoch: 75 [400/1248 (32%)] Loss: 32.356037
Train Epoch: 75 [450/1248 (36%)] Loss: 30.297743
Train Epoch: 75 [500/1248 (40%)] Loss: 29.757708
Train Epoch: 75 [550/1248 (44%)] Loss: 30.023739
Train Epoch: 75 [600/1248 (48%)] Loss: 30.873297
Train Epoch: 75 [650/1248 (52%)] Loss: 26.825630
Train Epoch: 75 [700/1248 (56%)] Loss: 32.960434
Train Epoch: 75 [750/1248 (60%)] Loss: 30.495838
Train Epoch: 75 [800/1248 (64%)] Loss: 31.801809
Train Epoch: 75 [850/1248 (68%)] Loss: 32.977654
Train Epoch: 75 [900/1248 (72%)] Loss: 32.503670
Train Epoch: 75 [950/1248 (76%)] Loss: 31.813095
Train Epoch: 75 [1000/1248 (80%)] Loss: 32.384525
Train Epoch: 75 [1050/1248 (84%)] Loss: 30.754515
Train Epoch: 75 [1100/1248 (88%)] Loss: 31.252045
Train Epoch: 75 [1150/1248 (92%)] Loss: 32.283436
Train Epoch: 75 [1200/1248 (96%)] Loss: 31.480400
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  6.02it/s]
    epoch          : 75
    loss           : 31.26913730621338
    grad_norm      : 2788.2832495117186
    val_loss       : 27.906782740311655
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch75.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:20<00:00,  3.89it/s]
Train Epoch: 76 [0/1248 (0%)] Loss: 29.302662
Train Epoch: 76 [50/1248 (4%)] Loss: 29.971863
Train Epoch: 76 [100/1248 (8%)] Loss: 30.235435
Train Epoch: 76 [150/1248 (12%)] Loss: 29.769047
Train Epoch: 76 [200/1248 (16%)] Loss: 29.954884
Train Epoch: 76 [250/1248 (20%)] Loss: 31.620890
Train Epoch: 76 [300/1248 (24%)] Loss: 32.455124
Train Epoch: 76 [350/1248 (28%)] Loss: 29.281115
Train Epoch: 76 [400/1248 (32%)] Loss: 28.702763
Train Epoch: 76 [450/1248 (36%)] Loss: 29.036608
Train Epoch: 76 [500/1248 (40%)] Loss: 32.939369
Train Epoch: 76 [550/1248 (44%)] Loss: 31.497953
Train Epoch: 76 [600/1248 (48%)] Loss: 31.186874
Train Epoch: 76 [650/1248 (52%)] Loss: 32.119987
Train Epoch: 76 [700/1248 (56%)] Loss: 29.992596
Train Epoch: 76 [750/1248 (60%)] Loss: 30.786329
Train Epoch: 76 [800/1248 (64%)] Loss: 31.845800
Train Epoch: 76 [850/1248 (68%)] Loss: 31.627090
Train Epoch: 76 [900/1248 (72%)] Loss: 30.772779
Train Epoch: 76 [950/1248 (76%)] Loss: 30.813217
Train Epoch: 76 [1000/1248 (80%)] Loss: 31.502401
Train Epoch: 76 [1050/1248 (84%)] Loss: 33.765961
Train Epoch: 76 [1100/1248 (88%)] Loss: 32.711300
Train Epoch: 76 [1150/1248 (92%)] Loss: 32.175186
Train Epoch: 76 [1200/1248 (96%)] Loss: 33.663303
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:24<00:00,  5.77it/s]
    epoch          : 76
    loss           : 31.343453178405763
    grad_norm      : 3819.4050708007812
    val_loss       : 28.73336720638138
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch76.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:26<00:00,  3.81it/s]
Train Epoch: 77 [0/1248 (0%)] Loss: 34.103378
Train Epoch: 77 [50/1248 (4%)] Loss: 31.476816
Train Epoch: 77 [100/1248 (8%)] Loss: 30.875582
Train Epoch: 77 [150/1248 (12%)] Loss: 31.284735
Train Epoch: 77 [200/1248 (16%)] Loss: 30.021839
Train Epoch: 77 [250/1248 (20%)] Loss: 31.289499
Train Epoch: 77 [300/1248 (24%)] Loss: 31.275373
Train Epoch: 77 [350/1248 (28%)] Loss: 31.381865
Train Epoch: 77 [400/1248 (32%)] Loss: 31.826290
Train Epoch: 77 [450/1248 (36%)] Loss: 30.829914
Train Epoch: 77 [500/1248 (40%)] Loss: 32.843716
Train Epoch: 77 [550/1248 (44%)] Loss: 33.283550
Train Epoch: 77 [600/1248 (48%)] Loss: 33.243610
Train Epoch: 77 [650/1248 (52%)] Loss: 31.006208
Train Epoch: 77 [700/1248 (56%)] Loss: 30.541763
Train Epoch: 77 [750/1248 (60%)] Loss: 32.539597
Train Epoch: 77 [800/1248 (64%)] Loss: 28.937456
Train Epoch: 77 [850/1248 (68%)] Loss: 31.489330
Train Epoch: 77 [900/1248 (72%)] Loss: 28.174528
Train Epoch: 77 [950/1248 (76%)] Loss: 31.502413
Train Epoch: 77 [1000/1248 (80%)] Loss: 31.630922
Train Epoch: 77 [1050/1248 (84%)] Loss: 30.372616
Train Epoch: 77 [1100/1248 (88%)] Loss: 29.350105
Train Epoch: 77 [1150/1248 (92%)] Loss: 32.455936
Train Epoch: 77 [1200/1248 (96%)] Loss: 30.626783
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.09it/s]
    epoch          : 77
    loss           : 30.347730293273926
    grad_norm      : 3576.96154296875
    val_loss       : 27.68386901196816
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch77.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:20<00:00,  3.90it/s]
Train Epoch: 78 [0/1248 (0%)] Loss: 31.652822
Train Epoch: 78 [50/1248 (4%)] Loss: 31.414045
Train Epoch: 78 [100/1248 (8%)] Loss: 31.189461
Train Epoch: 78 [150/1248 (12%)] Loss: 32.023254
Train Epoch: 78 [200/1248 (16%)] Loss: 29.036192
Train Epoch: 78 [250/1248 (20%)] Loss: 33.273289
Train Epoch: 78 [300/1248 (24%)] Loss: 32.421360
Train Epoch: 78 [350/1248 (28%)] Loss: 32.116543
Train Epoch: 78 [400/1248 (32%)] Loss: 33.935703
Train Epoch: 78 [450/1248 (36%)] Loss: 32.319740
Train Epoch: 78 [500/1248 (40%)] Loss: 34.187180
Train Epoch: 78 [550/1248 (44%)] Loss: 31.955647
Train Epoch: 78 [600/1248 (48%)] Loss: 30.388762
Train Epoch: 78 [650/1248 (52%)] Loss: 30.123123
Train Epoch: 78 [700/1248 (56%)] Loss: 32.442467
Train Epoch: 78 [750/1248 (60%)] Loss: 34.093227
Train Epoch: 78 [800/1248 (64%)] Loss: 30.868900
Train Epoch: 78 [850/1248 (68%)] Loss: 30.159483
Train Epoch: 78 [900/1248 (72%)] Loss: 31.164978
Train Epoch: 78 [950/1248 (76%)] Loss: 29.620983
Train Epoch: 78 [1000/1248 (80%)] Loss: 33.492699
Train Epoch: 78 [1050/1248 (84%)] Loss: 32.144024
Train Epoch: 78 [1100/1248 (88%)] Loss: 30.931004
Train Epoch: 78 [1150/1248 (92%)] Loss: 31.923939
Train Epoch: 78 [1200/1248 (96%)] Loss: 32.888096
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.14it/s]
    epoch          : 78
    loss           : 30.71672691345215
    grad_norm      : 3178.002449951172
    val_loss       : 27.28512239799225
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch78.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:30<00:00,  3.78it/s]
Train Epoch: 79 [0/1248 (0%)] Loss: 30.967941
Train Epoch: 79 [50/1248 (4%)] Loss: 30.236729
Train Epoch: 79 [100/1248 (8%)] Loss: 29.069658
Train Epoch: 79 [150/1248 (12%)] Loss: 32.050102
Train Epoch: 79 [200/1248 (16%)] Loss: 30.588966
Train Epoch: 79 [250/1248 (20%)] Loss: 30.264893
Train Epoch: 79 [300/1248 (24%)] Loss: 29.469255
Train Epoch: 79 [350/1248 (28%)] Loss: 34.073399
Train Epoch: 79 [400/1248 (32%)] Loss: 32.349236
Train Epoch: 79 [450/1248 (36%)] Loss: 31.479073
Train Epoch: 79 [500/1248 (40%)] Loss: 32.865505
Train Epoch: 79 [550/1248 (44%)] Loss: 31.195036
Train Epoch: 79 [600/1248 (48%)] Loss: 32.732754
Train Epoch: 79 [650/1248 (52%)] Loss: 32.657349
Train Epoch: 79 [700/1248 (56%)] Loss: 32.369858
Train Epoch: 79 [750/1248 (60%)] Loss: 30.973820
Train Epoch: 79 [800/1248 (64%)] Loss: 33.104759
Train Epoch: 79 [850/1248 (68%)] Loss: 30.928234
Train Epoch: 79 [900/1248 (72%)] Loss: 28.380842
Train Epoch: 79 [950/1248 (76%)] Loss: 31.711344
Train Epoch: 79 [1000/1248 (80%)] Loss: 31.083342
Train Epoch: 79 [1050/1248 (84%)] Loss: 30.924938
Train Epoch: 79 [1100/1248 (88%)] Loss: 30.355844
Train Epoch: 79 [1150/1248 (92%)] Loss: 34.407829
Train Epoch: 79 [1200/1248 (96%)] Loss: 32.858826
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.91it/s]
    epoch          : 79
    loss           : 31.24974563598633
    grad_norm      : 2304.274891357422
    val_loss       : 26.915970177959196
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch79.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:28<00:00,  3.79it/s]
Train Epoch: 80 [0/1248 (0%)] Loss: 31.192194
Train Epoch: 80 [50/1248 (4%)] Loss: 33.063454
Train Epoch: 80 [100/1248 (8%)] Loss: 29.905621
Train Epoch: 80 [150/1248 (12%)] Loss: 29.270189
Train Epoch: 80 [200/1248 (16%)] Loss: 33.533451
Train Epoch: 80 [250/1248 (20%)] Loss: 30.815731
Train Epoch: 80 [300/1248 (24%)] Loss: 29.961109
Train Epoch: 80 [350/1248 (28%)] Loss: 28.466873
Train Epoch: 80 [400/1248 (32%)] Loss: 27.354376
Train Epoch: 80 [450/1248 (36%)] Loss: 29.910509
Train Epoch: 80 [500/1248 (40%)] Loss: 35.154327
Train Epoch: 80 [550/1248 (44%)] Loss: 31.018517
Train Epoch: 80 [600/1248 (48%)] Loss: 31.823267
Train Epoch: 80 [650/1248 (52%)] Loss: 31.983429
Train Epoch: 80 [700/1248 (56%)] Loss: 32.082748
Train Epoch: 80 [750/1248 (60%)] Loss: 31.654276
Train Epoch: 80 [800/1248 (64%)] Loss: 29.893749
Train Epoch: 80 [850/1248 (68%)] Loss: 30.526056
Train Epoch: 80 [900/1248 (72%)] Loss: 32.501537
Train Epoch: 80 [950/1248 (76%)] Loss: 31.195982
Train Epoch: 80 [1000/1248 (80%)] Loss: 28.569593
Train Epoch: 80 [1050/1248 (84%)] Loss: 31.609474
Train Epoch: 80 [1100/1248 (88%)] Loss: 32.184444
Train Epoch: 80 [1150/1248 (92%)] Loss: 31.222614
Train Epoch: 80 [1200/1248 (96%)] Loss: 29.387991
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.95it/s]
    epoch          : 80
    loss           : 31.03245361328125
    grad_norm      : 3467.3116015625
    val_loss       : 27.728538046637883
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch80.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:26<00:00,  3.82it/s]
Train Epoch: 81 [0/1248 (0%)] Loss: 30.249929
Train Epoch: 81 [50/1248 (4%)] Loss: 32.341610
Train Epoch: 81 [100/1248 (8%)] Loss: 30.075014
Train Epoch: 81 [150/1248 (12%)] Loss: 33.408165
Train Epoch: 81 [200/1248 (16%)] Loss: 29.699818
Train Epoch: 81 [250/1248 (20%)] Loss: 30.770369
Train Epoch: 81 [300/1248 (24%)] Loss: 30.477907
Train Epoch: 81 [350/1248 (28%)] Loss: 30.675907
Train Epoch: 81 [400/1248 (32%)] Loss: 29.681284
Train Epoch: 81 [450/1248 (36%)] Loss: 33.032272
Train Epoch: 81 [500/1248 (40%)] Loss: 31.312538
Train Epoch: 81 [550/1248 (44%)] Loss: 32.443066
Train Epoch: 81 [600/1248 (48%)] Loss: 31.268702
Train Epoch: 81 [650/1248 (52%)] Loss: 33.477757
Train Epoch: 81 [700/1248 (56%)] Loss: 29.661549
Train Epoch: 81 [750/1248 (60%)] Loss: 32.287930
Train Epoch: 81 [800/1248 (64%)] Loss: 30.496498
Train Epoch: 81 [850/1248 (68%)] Loss: 31.228558
Train Epoch: 81 [900/1248 (72%)] Loss: 31.258886
Train Epoch: 81 [950/1248 (76%)] Loss: 29.561930
Train Epoch: 81 [1000/1248 (80%)] Loss: 31.384384
Train Epoch: 81 [1050/1248 (84%)] Loss: 33.150345
Train Epoch: 81 [1100/1248 (88%)] Loss: 30.190735
Train Epoch: 81 [1150/1248 (92%)] Loss: 33.358624
Train Epoch: 81 [1200/1248 (96%)] Loss: 31.316730
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.15it/s]
    epoch          : 81
    loss           : 31.293269538879393
    grad_norm      : 2847.2999462890625
    val_loss       : 28.93012546292312
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch81.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:26<00:00,  3.82it/s]
Train Epoch: 82 [0/1248 (0%)] Loss: 31.493933
Train Epoch: 82 [50/1248 (4%)] Loss: 31.291151
Train Epoch: 82 [100/1248 (8%)] Loss: 33.156700
Train Epoch: 82 [150/1248 (12%)] Loss: 31.913208
Train Epoch: 82 [200/1248 (16%)] Loss: 31.756134
Train Epoch: 82 [250/1248 (20%)] Loss: 28.670828
Train Epoch: 82 [300/1248 (24%)] Loss: 30.379086
Train Epoch: 82 [350/1248 (28%)] Loss: 31.634449
Train Epoch: 82 [400/1248 (32%)] Loss: 31.307508
Train Epoch: 82 [450/1248 (36%)] Loss: 30.495440
Train Epoch: 82 [500/1248 (40%)] Loss: 29.184910
Train Epoch: 82 [550/1248 (44%)] Loss: 33.182549
Train Epoch: 82 [600/1248 (48%)] Loss: 33.625195
Train Epoch: 82 [650/1248 (52%)] Loss: 32.464687
Train Epoch: 82 [700/1248 (56%)] Loss: 33.921555
Train Epoch: 82 [750/1248 (60%)] Loss: 32.501682
Train Epoch: 82 [800/1248 (64%)] Loss: 33.230118
Train Epoch: 82 [850/1248 (68%)] Loss: 31.952778
Train Epoch: 82 [900/1248 (72%)] Loss: 30.381630
Train Epoch: 82 [950/1248 (76%)] Loss: 33.758583
Train Epoch: 82 [1000/1248 (80%)] Loss: 31.798254
Train Epoch: 82 [1050/1248 (84%)] Loss: 31.395592
Train Epoch: 82 [1100/1248 (88%)] Loss: 32.100479
Train Epoch: 82 [1150/1248 (92%)] Loss: 30.867046
Train Epoch: 82 [1200/1248 (96%)] Loss: 31.659920
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.88it/s]
    epoch          : 82
    loss           : 30.45640148162842
    grad_norm      : 3604.0101904296876
    val_loss       : 27.940740105059508
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch82.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:28<00:00,  3.79it/s]
Train Epoch: 83 [0/1248 (0%)] Loss: 31.695198
Train Epoch: 83 [50/1248 (4%)] Loss: 32.215607
Train Epoch: 83 [100/1248 (8%)] Loss: 28.181942
Train Epoch: 83 [150/1248 (12%)] Loss: 26.926657
Train Epoch: 83 [200/1248 (16%)] Loss: 31.058846
Train Epoch: 83 [250/1248 (20%)] Loss: 30.546602
Train Epoch: 83 [300/1248 (24%)] Loss: 31.875505
Train Epoch: 83 [350/1248 (28%)] Loss: 32.329411
Train Epoch: 83 [400/1248 (32%)] Loss: 31.491007
Train Epoch: 83 [450/1248 (36%)] Loss: 32.123646
Train Epoch: 83 [500/1248 (40%)] Loss: 30.091846
Train Epoch: 83 [550/1248 (44%)] Loss: 32.564854
Train Epoch: 83 [600/1248 (48%)] Loss: 30.627872
Train Epoch: 83 [650/1248 (52%)] Loss: 30.462433
Train Epoch: 83 [700/1248 (56%)] Loss: 30.602905
Train Epoch: 83 [750/1248 (60%)] Loss: 31.360691
Train Epoch: 83 [800/1248 (64%)] Loss: 32.738846
Train Epoch: 83 [850/1248 (68%)] Loss: 30.241030
Train Epoch: 83 [900/1248 (72%)] Loss: 31.527363
Train Epoch: 83 [950/1248 (76%)] Loss: 34.547283
Train Epoch: 83 [1000/1248 (80%)] Loss: 31.376303
Train Epoch: 83 [1050/1248 (84%)] Loss: 31.530840
Train Epoch: 83 [1100/1248 (88%)] Loss: 32.005524
Train Epoch: 83 [1150/1248 (92%)] Loss: 28.313742
Train Epoch: 83 [1200/1248 (96%)] Loss: 32.253216
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.90it/s]
    epoch          : 83
    loss           : 30.868963508605958
    grad_norm      : 3004.8842700195314
    val_loss       : 27.702669637666332
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch83.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:26<00:00,  3.81it/s]
Train Epoch: 84 [0/1248 (0%)] Loss: 32.375519
Train Epoch: 84 [50/1248 (4%)] Loss: 33.309826
Train Epoch: 84 [100/1248 (8%)] Loss: 28.802816
Train Epoch: 84 [150/1248 (12%)] Loss: 32.234222
Train Epoch: 84 [200/1248 (16%)] Loss: 30.269186
Train Epoch: 84 [250/1248 (20%)] Loss: 32.720650
Train Epoch: 84 [300/1248 (24%)] Loss: 31.664804
Train Epoch: 84 [350/1248 (28%)] Loss: 27.421295
Train Epoch: 84 [400/1248 (32%)] Loss: 30.215181
Train Epoch: 84 [450/1248 (36%)] Loss: 30.029907
Train Epoch: 84 [500/1248 (40%)] Loss: 30.432028
Train Epoch: 84 [550/1248 (44%)] Loss: 30.977034
Train Epoch: 84 [600/1248 (48%)] Loss: 30.435417
Train Epoch: 84 [650/1248 (52%)] Loss: 32.605572
Train Epoch: 84 [700/1248 (56%)] Loss: 32.845596
Train Epoch: 84 [750/1248 (60%)] Loss: 30.362648
Train Epoch: 84 [800/1248 (64%)] Loss: 27.795835
Train Epoch: 84 [850/1248 (68%)] Loss: 26.010872
Train Epoch: 84 [900/1248 (72%)] Loss: 33.900124
Train Epoch: 84 [950/1248 (76%)] Loss: 31.358395
Train Epoch: 84 [1000/1248 (80%)] Loss: 30.525555
Train Epoch: 84 [1050/1248 (84%)] Loss: 28.559559
Train Epoch: 84 [1100/1248 (88%)] Loss: 30.725679
Train Epoch: 84 [1150/1248 (92%)] Loss: 30.300697
Train Epoch: 84 [1200/1248 (96%)] Loss: 32.131989
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.20it/s]
    epoch          : 84
    loss           : 31.577005729675292
    grad_norm      : 2754.6218603515626
    val_loss       : 27.18195384183376
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch84.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:27<00:00,  3.81it/s]
Train Epoch: 85 [0/1248 (0%)] Loss: 32.515305
Train Epoch: 85 [50/1248 (4%)] Loss: 31.723099
Train Epoch: 85 [100/1248 (8%)] Loss: 31.028622
Train Epoch: 85 [150/1248 (12%)] Loss: 29.802269
Train Epoch: 85 [200/1248 (16%)] Loss: 30.912897
Train Epoch: 85 [250/1248 (20%)] Loss: 29.735046
Train Epoch: 85 [300/1248 (24%)] Loss: 33.568470
Train Epoch: 85 [350/1248 (28%)] Loss: 29.560747
Train Epoch: 85 [400/1248 (32%)] Loss: 31.783333
Train Epoch: 85 [450/1248 (36%)] Loss: 29.693232
Train Epoch: 85 [500/1248 (40%)] Loss: 30.464716
Train Epoch: 85 [550/1248 (44%)] Loss: 30.240122
Train Epoch: 85 [600/1248 (48%)] Loss: 33.056545
Train Epoch: 85 [650/1248 (52%)] Loss: 29.571074
Train Epoch: 85 [700/1248 (56%)] Loss: 31.394070
Train Epoch: 85 [750/1248 (60%)] Loss: 33.366982
Train Epoch: 85 [800/1248 (64%)] Loss: 29.998968
Train Epoch: 85 [850/1248 (68%)] Loss: 30.571619
Train Epoch: 85 [900/1248 (72%)] Loss: 31.484367
Train Epoch: 85 [950/1248 (76%)] Loss: 29.981369
Train Epoch: 85 [1000/1248 (80%)] Loss: 29.549019
Train Epoch: 85 [1050/1248 (84%)] Loss: 33.025558
Train Epoch: 85 [1100/1248 (88%)] Loss: 29.600128
Train Epoch: 85 [1150/1248 (92%)] Loss: 31.231674
Train Epoch: 85 [1200/1248 (96%)] Loss: 31.690044
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  6.02it/s]
    epoch          : 85
    loss           : 31.14783634185791
    grad_norm      : 2236.8228002929686
    val_loss       : 26.845679084174066
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch85.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:28<00:00,  3.79it/s]
Train Epoch: 86 [0/1248 (0%)] Loss: 31.925760
Train Epoch: 86 [50/1248 (4%)] Loss: 29.203671
Train Epoch: 86 [100/1248 (8%)] Loss: 29.287886
Train Epoch: 86 [150/1248 (12%)] Loss: 30.696762
Train Epoch: 86 [200/1248 (16%)] Loss: 32.066273
Train Epoch: 86 [250/1248 (20%)] Loss: 31.316738
Train Epoch: 86 [300/1248 (24%)] Loss: 30.006681
Train Epoch: 86 [350/1248 (28%)] Loss: 31.663124
Train Epoch: 86 [400/1248 (32%)] Loss: 31.835278
Train Epoch: 86 [450/1248 (36%)] Loss: 32.054779
Train Epoch: 86 [500/1248 (40%)] Loss: 32.545906
Train Epoch: 86 [550/1248 (44%)] Loss: 31.791748
Train Epoch: 86 [600/1248 (48%)] Loss: 28.816151
Train Epoch: 86 [650/1248 (52%)] Loss: 30.731140
Train Epoch: 86 [700/1248 (56%)] Loss: 32.885925
Train Epoch: 86 [750/1248 (60%)] Loss: 31.608402
Train Epoch: 86 [800/1248 (64%)] Loss: 30.326303
Train Epoch: 86 [850/1248 (68%)] Loss: 31.433952
Train Epoch: 86 [900/1248 (72%)] Loss: 30.177452
Train Epoch: 86 [950/1248 (76%)] Loss: 31.329538
Train Epoch: 86 [1000/1248 (80%)] Loss: 33.090225
Train Epoch: 86 [1050/1248 (84%)] Loss: 31.583151
Train Epoch: 86 [1100/1248 (88%)] Loss: 28.114981
Train Epoch: 86 [1150/1248 (92%)] Loss: 31.117676
Train Epoch: 86 [1200/1248 (96%)] Loss: 28.722481
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  6.03it/s]
    epoch          : 86
    loss           : 30.787975311279297
    grad_norm      : 3552.900555419922
    val_loss       : 27.379757984079045
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch86.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:26<00:00,  3.81it/s]
Train Epoch: 87 [0/1248 (0%)] Loss: 32.042423
Train Epoch: 87 [50/1248 (4%)] Loss: 29.082613
Train Epoch: 87 [100/1248 (8%)] Loss: 30.955593
Train Epoch: 87 [150/1248 (12%)] Loss: 32.099506
Train Epoch: 87 [200/1248 (16%)] Loss: 33.242283
Train Epoch: 87 [250/1248 (20%)] Loss: 30.024792
Train Epoch: 87 [300/1248 (24%)] Loss: 28.200409
Train Epoch: 87 [350/1248 (28%)] Loss: 29.474741
Train Epoch: 87 [400/1248 (32%)] Loss: 30.689623
Train Epoch: 87 [450/1248 (36%)] Loss: 31.523466
Train Epoch: 87 [500/1248 (40%)] Loss: 28.497810
Train Epoch: 87 [550/1248 (44%)] Loss: 30.305012
Train Epoch: 87 [600/1248 (48%)] Loss: 31.974003
Train Epoch: 87 [650/1248 (52%)] Loss: 27.760735
Train Epoch: 87 [700/1248 (56%)] Loss: 30.451645
Train Epoch: 87 [750/1248 (60%)] Loss: 32.529869
Train Epoch: 87 [800/1248 (64%)] Loss: 28.376242
Train Epoch: 87 [850/1248 (68%)] Loss: 32.375076
Train Epoch: 87 [900/1248 (72%)] Loss: 32.535275
Train Epoch: 87 [950/1248 (76%)] Loss: 31.401274
Train Epoch: 87 [1000/1248 (80%)] Loss: 31.705715
Train Epoch: 87 [1050/1248 (84%)] Loss: 30.741772
Train Epoch: 87 [1100/1248 (88%)] Loss: 32.946957
Train Epoch: 87 [1150/1248 (92%)] Loss: 30.617596
Train Epoch: 87 [1200/1248 (96%)] Loss: 30.823597
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.19it/s]
    epoch          : 87
    loss           : 31.546727867126464
    grad_norm      : 3136.925607910156
    val_loss       : 29.161935545557693
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch87.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:22<00:00,  3.86it/s]
Train Epoch: 88 [0/1248 (0%)] Loss: 31.001812
Train Epoch: 88 [50/1248 (4%)] Loss: 33.032726
Train Epoch: 88 [100/1248 (8%)] Loss: 33.865032
Train Epoch: 88 [150/1248 (12%)] Loss: 29.407846
Train Epoch: 88 [200/1248 (16%)] Loss: 31.587713
Train Epoch: 88 [250/1248 (20%)] Loss: 33.197655
Train Epoch: 88 [300/1248 (24%)] Loss: 29.814722
Train Epoch: 88 [350/1248 (28%)] Loss: 30.231510
Train Epoch: 88 [400/1248 (32%)] Loss: 31.164194
Train Epoch: 88 [450/1248 (36%)] Loss: 27.228542
Train Epoch: 88 [500/1248 (40%)] Loss: 30.791927
Train Epoch: 88 [550/1248 (44%)] Loss: 31.819727
Train Epoch: 88 [600/1248 (48%)] Loss: 31.255602
Train Epoch: 88 [650/1248 (52%)] Loss: 28.947098
Train Epoch: 88 [700/1248 (56%)] Loss: 31.744223
Train Epoch: 88 [750/1248 (60%)] Loss: 31.994095
Train Epoch: 88 [800/1248 (64%)] Loss: 30.695984
Train Epoch: 88 [850/1248 (68%)] Loss: 32.980671
Train Epoch: 88 [900/1248 (72%)] Loss: 28.929747
Train Epoch: 88 [950/1248 (76%)] Loss: 31.273499
Train Epoch: 88 [1000/1248 (80%)] Loss: 29.852648
Train Epoch: 88 [1050/1248 (84%)] Loss: 30.919790
Train Epoch: 88 [1100/1248 (88%)] Loss: 31.668425
Train Epoch: 88 [1150/1248 (92%)] Loss: 31.935644
Train Epoch: 88 [1200/1248 (96%)] Loss: 31.484715
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.14it/s]
    epoch          : 88
    loss           : 31.25484432220459
    grad_norm      : 3120.753381347656
    val_loss       : 28.096462977018287
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch88.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:20<00:00,  3.89it/s]
Train Epoch: 89 [0/1248 (0%)] Loss: 30.229246
Train Epoch: 89 [50/1248 (4%)] Loss: 29.847679
Train Epoch: 89 [100/1248 (8%)] Loss: 31.568583
Train Epoch: 89 [150/1248 (12%)] Loss: 29.677301
Train Epoch: 89 [200/1248 (16%)] Loss: 32.686871
Train Epoch: 89 [250/1248 (20%)] Loss: 29.465349
Train Epoch: 89 [300/1248 (24%)] Loss: 32.652214
Train Epoch: 89 [350/1248 (28%)] Loss: 34.870270
Train Epoch: 89 [400/1248 (32%)] Loss: 31.779284
Train Epoch: 89 [450/1248 (36%)] Loss: 31.947975
Train Epoch: 89 [500/1248 (40%)] Loss: 26.704662
Train Epoch: 89 [550/1248 (44%)] Loss: 30.798788
Train Epoch: 89 [600/1248 (48%)] Loss: 30.024727
Train Epoch: 89 [650/1248 (52%)] Loss: 32.057713
Train Epoch: 89 [700/1248 (56%)] Loss: 30.971201
Train Epoch: 89 [750/1248 (60%)] Loss: 32.240524
Train Epoch: 89 [800/1248 (64%)] Loss: 31.274240
Train Epoch: 89 [850/1248 (68%)] Loss: 32.811996
Train Epoch: 89 [900/1248 (72%)] Loss: 33.405922
Train Epoch: 89 [950/1248 (76%)] Loss: 31.987520
Train Epoch: 89 [1000/1248 (80%)] Loss: 32.074772
Train Epoch: 89 [1050/1248 (84%)] Loss: 28.877945
Train Epoch: 89 [1100/1248 (88%)] Loss: 32.216309
Train Epoch: 89 [1150/1248 (92%)] Loss: 30.497730
Train Epoch: 89 [1200/1248 (96%)] Loss: 31.851059
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.17it/s]
    epoch          : 89
    loss           : 31.32868045806885
    grad_norm      : 3646.8407934570314
    val_loss       : 27.875075223634568
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch89.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:22<00:00,  3.87it/s]
Train Epoch: 90 [0/1248 (0%)] Loss: 31.217918
Train Epoch: 90 [50/1248 (4%)] Loss: 31.613541
Train Epoch: 90 [100/1248 (8%)] Loss: 31.231096
Train Epoch: 90 [150/1248 (12%)] Loss: 31.720705
Train Epoch: 90 [200/1248 (16%)] Loss: 32.325840
Train Epoch: 90 [250/1248 (20%)] Loss: 30.737762
Train Epoch: 90 [300/1248 (24%)] Loss: 29.172222
Train Epoch: 90 [350/1248 (28%)] Loss: 32.159344
Train Epoch: 90 [400/1248 (32%)] Loss: 29.284214
Train Epoch: 90 [450/1248 (36%)] Loss: 30.575497
Train Epoch: 90 [500/1248 (40%)] Loss: 31.863064
Train Epoch: 90 [550/1248 (44%)] Loss: 33.770031
Train Epoch: 90 [600/1248 (48%)] Loss: 29.550892
Train Epoch: 90 [650/1248 (52%)] Loss: 29.997829
Train Epoch: 90 [700/1248 (56%)] Loss: 31.727821
Train Epoch: 90 [750/1248 (60%)] Loss: 34.534706
Train Epoch: 90 [800/1248 (64%)] Loss: 27.697697
Train Epoch: 90 [850/1248 (68%)] Loss: 30.690704
Train Epoch: 90 [900/1248 (72%)] Loss: 30.687296
Train Epoch: 90 [950/1248 (76%)] Loss: 30.956999
Train Epoch: 90 [1000/1248 (80%)] Loss: 30.275290
Train Epoch: 90 [1050/1248 (84%)] Loss: 34.903992
Train Epoch: 90 [1100/1248 (88%)] Loss: 30.186584
Train Epoch: 90 [1150/1248 (92%)] Loss: 28.640240
Train Epoch: 90 [1200/1248 (96%)] Loss: 32.628975
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  6.00it/s]
    epoch          : 90
    loss           : 31.309818954467772
    grad_norm      : 2646.6946740722656
    val_loss       : 27.232045441222706
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch90.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:33<00:00,  3.74it/s]
Train Epoch: 91 [0/1248 (0%)] Loss: 29.731182
Train Epoch: 91 [50/1248 (4%)] Loss: 31.751913
Train Epoch: 91 [100/1248 (8%)] Loss: 32.097172
Train Epoch: 91 [150/1248 (12%)] Loss: 32.602509
Train Epoch: 91 [200/1248 (16%)] Loss: 32.818848
Train Epoch: 91 [250/1248 (20%)] Loss: 30.551048
Train Epoch: 91 [300/1248 (24%)] Loss: 31.609266
Train Epoch: 91 [350/1248 (28%)] Loss: 31.102171
Train Epoch: 91 [400/1248 (32%)] Loss: 31.368959
Train Epoch: 91 [450/1248 (36%)] Loss: 32.425583
Train Epoch: 91 [500/1248 (40%)] Loss: 30.386950
Train Epoch: 91 [550/1248 (44%)] Loss: 31.314131
Train Epoch: 91 [600/1248 (48%)] Loss: 28.791744
Train Epoch: 91 [650/1248 (52%)] Loss: 31.222288
Train Epoch: 91 [700/1248 (56%)] Loss: 29.122217
Train Epoch: 91 [750/1248 (60%)] Loss: 31.493990
Train Epoch: 91 [800/1248 (64%)] Loss: 31.948706
Train Epoch: 91 [850/1248 (68%)] Loss: 32.124413
Train Epoch: 91 [900/1248 (72%)] Loss: 29.490818
Train Epoch: 91 [950/1248 (76%)] Loss: 32.128624
Train Epoch: 91 [1000/1248 (80%)] Loss: 30.518764
Train Epoch: 91 [1050/1248 (84%)] Loss: 32.195820
Train Epoch: 91 [1100/1248 (88%)] Loss: 26.143206
Train Epoch: 91 [1150/1248 (92%)] Loss: 34.755909
Train Epoch: 91 [1200/1248 (96%)] Loss: 30.749254
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:24<00:00,  5.73it/s]
    epoch          : 91
    loss           : 31.38449634552002
    grad_norm      : 3167.515235595703
    val_loss       : 27.90777251703276
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch91.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:34<00:00,  3.73it/s]
Train Epoch: 92 [0/1248 (0%)] Loss: 31.900911
Train Epoch: 92 [50/1248 (4%)] Loss: 31.049141
Train Epoch: 92 [100/1248 (8%)] Loss: 35.192562
Train Epoch: 92 [150/1248 (12%)] Loss: 30.918535
Train Epoch: 92 [200/1248 (16%)] Loss: 32.382065
Train Epoch: 92 [250/1248 (20%)] Loss: 30.691168
Train Epoch: 92 [300/1248 (24%)] Loss: 28.966944
Train Epoch: 92 [350/1248 (28%)] Loss: 29.279274
Train Epoch: 92 [400/1248 (32%)] Loss: 29.910524
Train Epoch: 92 [450/1248 (36%)] Loss: 28.212221
Train Epoch: 92 [500/1248 (40%)] Loss: 31.460260
Train Epoch: 92 [550/1248 (44%)] Loss: 30.185369
Train Epoch: 92 [600/1248 (48%)] Loss: 33.823254
Train Epoch: 92 [650/1248 (52%)] Loss: 29.897451
Train Epoch: 92 [700/1248 (56%)] Loss: 30.296257
Train Epoch: 92 [750/1248 (60%)] Loss: 30.222485
Train Epoch: 92 [800/1248 (64%)] Loss: 32.588516
Train Epoch: 92 [850/1248 (68%)] Loss: 29.615206
Train Epoch: 92 [900/1248 (72%)] Loss: 30.263300
Train Epoch: 92 [950/1248 (76%)] Loss: 31.140886
Train Epoch: 92 [1000/1248 (80%)] Loss: 30.443893
Train Epoch: 92 [1050/1248 (84%)] Loss: 32.906334
Train Epoch: 92 [1100/1248 (88%)] Loss: 30.159128
Train Epoch: 92 [1150/1248 (92%)] Loss: 27.928413
Train Epoch: 92 [1200/1248 (96%)] Loss: 31.176037
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.06it/s]
    epoch          : 92
    loss           : 31.06015552520752
    grad_norm      : 3458.245838623047
    val_loss       : 28.307404497544542
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch92.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:23<00:00,  3.85it/s]
Train Epoch: 93 [0/1248 (0%)] Loss: 30.296484
Train Epoch: 93 [50/1248 (4%)] Loss: 31.111706
Train Epoch: 93 [100/1248 (8%)] Loss: 30.110355
Train Epoch: 93 [150/1248 (12%)] Loss: 29.977913
Train Epoch: 93 [200/1248 (16%)] Loss: 32.850510
Train Epoch: 93 [250/1248 (20%)] Loss: 28.312845
Train Epoch: 93 [300/1248 (24%)] Loss: 31.184071
Train Epoch: 93 [350/1248 (28%)] Loss: 29.579342
Train Epoch: 93 [400/1248 (32%)] Loss: 32.148361
Train Epoch: 93 [450/1248 (36%)] Loss: 32.107944
Train Epoch: 93 [500/1248 (40%)] Loss: 28.937054
Train Epoch: 93 [550/1248 (44%)] Loss: 31.970043
Train Epoch: 93 [600/1248 (48%)] Loss: 30.407383
Train Epoch: 93 [650/1248 (52%)] Loss: 29.648222
Train Epoch: 93 [700/1248 (56%)] Loss: 29.747131
Train Epoch: 93 [750/1248 (60%)] Loss: 27.796013
Train Epoch: 93 [800/1248 (64%)] Loss: 32.280453
Train Epoch: 93 [850/1248 (68%)] Loss: 32.816532
Train Epoch: 93 [900/1248 (72%)] Loss: 31.930037
Train Epoch: 93 [950/1248 (76%)] Loss: 32.726997
Train Epoch: 93 [1000/1248 (80%)] Loss: 29.889713
Train Epoch: 93 [1050/1248 (84%)] Loss: 30.608423
Train Epoch: 93 [1100/1248 (88%)] Loss: 29.800106
Train Epoch: 93 [1150/1248 (92%)] Loss: 33.235291
Train Epoch: 93 [1200/1248 (96%)] Loss: 31.787491
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.95it/s]
    epoch          : 93
    loss           : 31.028036499023436
    grad_norm      : 3729.4373608398437
    val_loss       : 27.411295609508485
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch93.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:32<00:00,  3.75it/s]
Train Epoch: 94 [0/1248 (0%)] Loss: 31.467127
Train Epoch: 94 [50/1248 (4%)] Loss: 32.532467
Train Epoch: 94 [100/1248 (8%)] Loss: 29.167065
Train Epoch: 94 [150/1248 (12%)] Loss: 32.376408
Train Epoch: 94 [200/1248 (16%)] Loss: 29.168201
Train Epoch: 94 [250/1248 (20%)] Loss: 31.320173
Train Epoch: 94 [300/1248 (24%)] Loss: 34.061825
Train Epoch: 94 [350/1248 (28%)] Loss: 30.493565
Train Epoch: 94 [400/1248 (32%)] Loss: 30.812368
Train Epoch: 94 [450/1248 (36%)] Loss: 31.912336
Train Epoch: 94 [500/1248 (40%)] Loss: 29.476789
Train Epoch: 94 [550/1248 (44%)] Loss: 30.342175
Train Epoch: 94 [600/1248 (48%)] Loss: 32.809963
Train Epoch: 94 [650/1248 (52%)] Loss: 32.156296
Train Epoch: 94 [700/1248 (56%)] Loss: 30.436172
Train Epoch: 94 [750/1248 (60%)] Loss: 29.923800
Train Epoch: 94 [800/1248 (64%)] Loss: 28.989685
Train Epoch: 94 [850/1248 (68%)] Loss: 32.475708
Train Epoch: 94 [900/1248 (72%)] Loss: 32.608597
Train Epoch: 94 [950/1248 (76%)] Loss: 30.844482
Train Epoch: 94 [1000/1248 (80%)] Loss: 29.344925
Train Epoch: 94 [1050/1248 (84%)] Loss: 33.474934
Train Epoch: 94 [1100/1248 (88%)] Loss: 32.214233
Train Epoch: 94 [1150/1248 (92%)] Loss: 30.558170
Train Epoch: 94 [1200/1248 (96%)] Loss: 29.959602
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  6.02it/s]
    epoch          : 94
    loss           : 31.24683979034424
    grad_norm      : 2907.393194580078
    val_loss       : 28.388724059509716
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch94.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:29<00:00,  3.79it/s]
Train Epoch: 95 [0/1248 (0%)] Loss: 29.602303
Train Epoch: 95 [50/1248 (4%)] Loss: 29.787306
Train Epoch: 95 [100/1248 (8%)] Loss: 33.789444
Train Epoch: 95 [150/1248 (12%)] Loss: 28.839497
Train Epoch: 95 [200/1248 (16%)] Loss: 31.011808
Train Epoch: 95 [250/1248 (20%)] Loss: 29.798136
Train Epoch: 95 [300/1248 (24%)] Loss: 29.477011
Train Epoch: 95 [350/1248 (28%)] Loss: 32.561016
Train Epoch: 95 [400/1248 (32%)] Loss: 28.884020
Train Epoch: 95 [450/1248 (36%)] Loss: 29.203236
Train Epoch: 95 [500/1248 (40%)] Loss: 28.303059
Train Epoch: 95 [550/1248 (44%)] Loss: 32.226425
Train Epoch: 95 [600/1248 (48%)] Loss: 31.690969
Train Epoch: 95 [650/1248 (52%)] Loss: 34.371311
Train Epoch: 95 [700/1248 (56%)] Loss: 32.320107
Train Epoch: 95 [750/1248 (60%)] Loss: 31.434586
Train Epoch: 95 [800/1248 (64%)] Loss: 32.799133
Train Epoch: 95 [850/1248 (68%)] Loss: 33.317989
Train Epoch: 95 [900/1248 (72%)] Loss: 31.871510
Train Epoch: 95 [950/1248 (76%)] Loss: 31.351892
Train Epoch: 95 [1000/1248 (80%)] Loss: 31.136984
Train Epoch: 95 [1050/1248 (84%)] Loss: 31.459074
Train Epoch: 95 [1100/1248 (88%)] Loss: 30.860952
Train Epoch: 95 [1150/1248 (92%)] Loss: 30.539677
Train Epoch: 95 [1200/1248 (96%)] Loss: 31.170319
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.13it/s]
    epoch          : 95
    loss           : 30.997592391967775
    grad_norm      : 2962.925079345703
    val_loss       : 27.442087694895353
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch95.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:25<00:00,  3.83it/s]
Train Epoch: 96 [0/1248 (0%)] Loss: 31.905174
Train Epoch: 96 [50/1248 (4%)] Loss: 29.956453
Train Epoch: 96 [100/1248 (8%)] Loss: 29.945295
Train Epoch: 96 [150/1248 (12%)] Loss: 31.339157
Train Epoch: 96 [200/1248 (16%)] Loss: 33.008034
Train Epoch: 96 [250/1248 (20%)] Loss: 30.080601
Train Epoch: 96 [300/1248 (24%)] Loss: 30.720375
Train Epoch: 96 [350/1248 (28%)] Loss: 29.634232
Train Epoch: 96 [400/1248 (32%)] Loss: 30.777512
Train Epoch: 96 [450/1248 (36%)] Loss: 30.155798
Train Epoch: 96 [500/1248 (40%)] Loss: 29.108149
Train Epoch: 96 [550/1248 (44%)] Loss: 30.210413
Train Epoch: 96 [600/1248 (48%)] Loss: 30.217888
Train Epoch: 96 [650/1248 (52%)] Loss: 33.174927
Train Epoch: 96 [700/1248 (56%)] Loss: 29.634356
Train Epoch: 96 [750/1248 (60%)] Loss: 32.250481
Train Epoch: 96 [800/1248 (64%)] Loss: 33.795658
Train Epoch: 96 [850/1248 (68%)] Loss: 30.415615
Train Epoch: 96 [900/1248 (72%)] Loss: 32.043457
Train Epoch: 96 [950/1248 (76%)] Loss: 31.709444
Train Epoch: 96 [1000/1248 (80%)] Loss: 30.905428
Train Epoch: 96 [1050/1248 (84%)] Loss: 32.177727
Train Epoch: 96 [1100/1248 (88%)] Loss: 32.154102
Train Epoch: 96 [1150/1248 (92%)] Loss: 29.535622
Train Epoch: 96 [1200/1248 (96%)] Loss: 30.150040
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.07it/s]
    epoch          : 96
    loss           : 31.415948028564454
    grad_norm      : 3394.3050842285156
    val_loss       : 28.046927486392235
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch96.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:29<00:00,  3.78it/s]
Train Epoch: 97 [0/1248 (0%)] Loss: 28.665880
Train Epoch: 97 [50/1248 (4%)] Loss: 30.672640
Train Epoch: 97 [100/1248 (8%)] Loss: 31.485569
Train Epoch: 97 [150/1248 (12%)] Loss: 32.956470
Train Epoch: 97 [200/1248 (16%)] Loss: 30.427525
Train Epoch: 97 [250/1248 (20%)] Loss: 32.465115
Train Epoch: 97 [300/1248 (24%)] Loss: 28.282967
Train Epoch: 97 [350/1248 (28%)] Loss: 30.390339
Train Epoch: 97 [400/1248 (32%)] Loss: 31.541323
Train Epoch: 97 [450/1248 (36%)] Loss: 31.982513
Train Epoch: 97 [500/1248 (40%)] Loss: 30.948635
Train Epoch: 97 [550/1248 (44%)] Loss: 28.898386
Train Epoch: 97 [600/1248 (48%)] Loss: 29.184153
Train Epoch: 97 [650/1248 (52%)] Loss: 28.930155
Train Epoch: 97 [700/1248 (56%)] Loss: 32.683296
Train Epoch: 97 [750/1248 (60%)] Loss: 33.652531
Train Epoch: 97 [800/1248 (64%)] Loss: 28.896582
Train Epoch: 97 [850/1248 (68%)] Loss: 32.840420
Train Epoch: 97 [900/1248 (72%)] Loss: 31.130703
Train Epoch: 97 [950/1248 (76%)] Loss: 29.267210
Train Epoch: 97 [1000/1248 (80%)] Loss: 32.666916
Train Epoch: 97 [1050/1248 (84%)] Loss: 31.066746
Train Epoch: 97 [1100/1248 (88%)] Loss: 32.363007
Train Epoch: 97 [1150/1248 (92%)] Loss: 33.225391
Train Epoch: 97 [1200/1248 (96%)] Loss: 31.078310
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.08it/s]
    epoch          : 97
    loss           : 31.397089042663573
    grad_norm      : 2914.3762939453127
    val_loss       : 29.16411386119376
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch97.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:27<00:00,  3.81it/s]
Train Epoch: 98 [0/1248 (0%)] Loss: 31.654869
Train Epoch: 98 [50/1248 (4%)] Loss: 30.079811
Train Epoch: 98 [100/1248 (8%)] Loss: 31.097925
Train Epoch: 98 [150/1248 (12%)] Loss: 33.236073
Train Epoch: 98 [200/1248 (16%)] Loss: 29.947254
Train Epoch: 98 [250/1248 (20%)] Loss: 32.173031
Train Epoch: 98 [300/1248 (24%)] Loss: 33.700001
Train Epoch: 98 [350/1248 (28%)] Loss: 30.642756
Train Epoch: 98 [400/1248 (32%)] Loss: 34.527149
Train Epoch: 98 [450/1248 (36%)] Loss: 34.113609
Train Epoch: 98 [500/1248 (40%)] Loss: 30.518974
Train Epoch: 98 [550/1248 (44%)] Loss: 33.477329
Train Epoch: 98 [600/1248 (48%)] Loss: 30.257511
Train Epoch: 98 [650/1248 (52%)] Loss: 30.878876
Train Epoch: 98 [700/1248 (56%)] Loss: 29.794527
Train Epoch: 98 [750/1248 (60%)] Loss: 29.259323
Train Epoch: 98 [800/1248 (64%)] Loss: 31.764126
Train Epoch: 98 [850/1248 (68%)] Loss: 32.520958
Train Epoch: 98 [900/1248 (72%)] Loss: 31.023706
Train Epoch: 98 [950/1248 (76%)] Loss: 32.224548
Train Epoch: 98 [1000/1248 (80%)] Loss: 29.684637
Train Epoch: 98 [1050/1248 (84%)] Loss: 28.129717
Train Epoch: 98 [1100/1248 (88%)] Loss: 30.990284
Train Epoch: 98 [1150/1248 (92%)] Loss: 29.933857
Train Epoch: 98 [1200/1248 (96%)] Loss: 31.280127
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.96it/s]
    epoch          : 98
    loss           : 31.526359367370606
    grad_norm      : 3320.8351092529297
    val_loss       : 27.852335442742
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch98.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:28<00:00,  3.80it/s]
Train Epoch: 99 [0/1248 (0%)] Loss: 29.618505
Train Epoch: 99 [50/1248 (4%)] Loss: 29.709864
Train Epoch: 99 [100/1248 (8%)] Loss: 33.237762
Train Epoch: 99 [150/1248 (12%)] Loss: 32.082798
Train Epoch: 99 [200/1248 (16%)] Loss: 32.767799
Train Epoch: 99 [250/1248 (20%)] Loss: 30.988188
Train Epoch: 99 [300/1248 (24%)] Loss: 33.375698
Train Epoch: 99 [350/1248 (28%)] Loss: 32.551743
Train Epoch: 99 [400/1248 (32%)] Loss: 32.070354
Train Epoch: 99 [450/1248 (36%)] Loss: 34.211060
Train Epoch: 99 [500/1248 (40%)] Loss: 30.579359
Train Epoch: 99 [550/1248 (44%)] Loss: 32.011433
Train Epoch: 99 [600/1248 (48%)] Loss: 27.779406
Train Epoch: 99 [650/1248 (52%)] Loss: 32.152279
Train Epoch: 99 [700/1248 (56%)] Loss: 33.049206
Train Epoch: 99 [750/1248 (60%)] Loss: 31.366190
Train Epoch: 99 [800/1248 (64%)] Loss: 31.435850
Train Epoch: 99 [850/1248 (68%)] Loss: 35.576199
Train Epoch: 99 [900/1248 (72%)] Loss: 29.728434
Train Epoch: 99 [950/1248 (76%)] Loss: 30.149719
Train Epoch: 99 [1000/1248 (80%)] Loss: 31.417271
Train Epoch: 99 [1050/1248 (84%)] Loss: 31.602093
Train Epoch: 99 [1100/1248 (88%)] Loss: 32.012955
Train Epoch: 99 [1150/1248 (92%)] Loss: 30.985010
Train Epoch: 99 [1200/1248 (96%)] Loss: 32.762272
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.19it/s]
    epoch          : 99
    loss           : 31.73068962097168
    grad_norm      : 3772.5910119628907
    val_loss       : 26.919133055981973
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch99.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:25<00:00,  3.83it/s]
Train Epoch: 100 [0/1248 (0%)] Loss: 28.032248
Train Epoch: 100 [50/1248 (4%)] Loss: 32.258141
Train Epoch: 100 [100/1248 (8%)] Loss: 31.031590
Train Epoch: 100 [150/1248 (12%)] Loss: 31.926105
Train Epoch: 100 [200/1248 (16%)] Loss: 31.511150
Train Epoch: 100 [250/1248 (20%)] Loss: 32.831272
Train Epoch: 100 [300/1248 (24%)] Loss: 33.342060
Train Epoch: 100 [350/1248 (28%)] Loss: 32.321556
Train Epoch: 100 [400/1248 (32%)] Loss: 33.375237
Train Epoch: 100 [450/1248 (36%)] Loss: 31.856024
Train Epoch: 100 [500/1248 (40%)] Loss: 33.312378
Train Epoch: 100 [550/1248 (44%)] Loss: 32.101250
Train Epoch: 100 [600/1248 (48%)] Loss: 30.722279
Train Epoch: 100 [650/1248 (52%)] Loss: 32.812263
Train Epoch: 100 [700/1248 (56%)] Loss: 32.825069
Train Epoch: 100 [750/1248 (60%)] Loss: 32.128983
Train Epoch: 100 [800/1248 (64%)] Loss: 25.488781
Train Epoch: 100 [850/1248 (68%)] Loss: 31.876972
Train Epoch: 100 [900/1248 (72%)] Loss: 31.517494
Train Epoch: 100 [950/1248 (76%)] Loss: 32.221550
Train Epoch: 100 [1000/1248 (80%)] Loss: 32.671108
Train Epoch: 100 [1050/1248 (84%)] Loss: 29.248524
Train Epoch: 100 [1100/1248 (88%)] Loss: 30.146496
Train Epoch: 100 [1150/1248 (92%)] Loss: 30.576941
Train Epoch: 100 [1200/1248 (96%)] Loss: 33.079018
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:22<00:00,  6.15it/s]
    epoch          : 100
    loss           : 31.245711708068846
    grad_norm      : 2394.7070153808595
    val_loss       : 28.20207132023873
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch100.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:25<00:00,  3.83it/s]
Train Epoch: 101 [0/1248 (0%)] Loss: 30.663149
Train Epoch: 101 [50/1248 (4%)] Loss: 34.303249
Train Epoch: 101 [100/1248 (8%)] Loss: 30.917706
Train Epoch: 101 [150/1248 (12%)] Loss: 29.114834
Train Epoch: 101 [200/1248 (16%)] Loss: 30.477762
Train Epoch: 101 [250/1248 (20%)] Loss: 34.201439
Train Epoch: 101 [300/1248 (24%)] Loss: 32.058872
Train Epoch: 101 [350/1248 (28%)] Loss: 32.336544
Train Epoch: 101 [400/1248 (32%)] Loss: 31.479731
Train Epoch: 101 [450/1248 (36%)] Loss: 33.605118
Train Epoch: 101 [500/1248 (40%)] Loss: 30.016342
Train Epoch: 101 [550/1248 (44%)] Loss: 32.115875
Train Epoch: 101 [600/1248 (48%)] Loss: 35.006374
Train Epoch: 101 [650/1248 (52%)] Loss: 31.968651
Train Epoch: 101 [700/1248 (56%)] Loss: 30.802691
Train Epoch: 101 [750/1248 (60%)] Loss: 32.372227
Train Epoch: 101 [800/1248 (64%)] Loss: 28.990807
Train Epoch: 101 [850/1248 (68%)] Loss: 32.683376
Train Epoch: 101 [900/1248 (72%)] Loss: 32.145481
Train Epoch: 101 [950/1248 (76%)] Loss: 31.927191
Train Epoch: 101 [1000/1248 (80%)] Loss: 31.751839
Train Epoch: 101 [1050/1248 (84%)] Loss: 32.202538
Train Epoch: 101 [1100/1248 (88%)] Loss: 30.959070
Train Epoch: 101 [1150/1248 (92%)] Loss: 32.065792
Train Epoch: 101 [1200/1248 (96%)] Loss: 27.878345
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:24<00:00,  5.73it/s]
    epoch          : 101
    loss           : 31.03887363433838
    grad_norm      : 3497.580443115234
    val_loss       : 27.177314415252464
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch101.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:36<00:00,  3.70it/s]
Train Epoch: 102 [0/1248 (0%)] Loss: 31.700111
Train Epoch: 102 [50/1248 (4%)] Loss: 29.633539
Train Epoch: 102 [100/1248 (8%)] Loss: 31.407728
Train Epoch: 102 [150/1248 (12%)] Loss: 33.599285
Train Epoch: 102 [200/1248 (16%)] Loss: 27.885431
Train Epoch: 102 [250/1248 (20%)] Loss: 32.392605
Train Epoch: 102 [300/1248 (24%)] Loss: 32.851654
Train Epoch: 102 [350/1248 (28%)] Loss: 29.945953
Train Epoch: 102 [400/1248 (32%)] Loss: 31.601383
Train Epoch: 102 [450/1248 (36%)] Loss: 30.955364
Train Epoch: 102 [500/1248 (40%)] Loss: 29.010685
Train Epoch: 102 [550/1248 (44%)] Loss: 34.915218
Train Epoch: 102 [600/1248 (48%)] Loss: 31.822235
Train Epoch: 102 [650/1248 (52%)] Loss: 31.707869
Train Epoch: 102 [700/1248 (56%)] Loss: 31.345299
Train Epoch: 102 [750/1248 (60%)] Loss: 28.933836
Train Epoch: 102 [800/1248 (64%)] Loss: 31.778976
Train Epoch: 102 [850/1248 (68%)] Loss: 34.142941
Train Epoch: 102 [900/1248 (72%)] Loss: 29.900675
Train Epoch: 102 [950/1248 (76%)] Loss: 32.308167
Train Epoch: 102 [1000/1248 (80%)] Loss: 32.079056
Train Epoch: 102 [1050/1248 (84%)] Loss: 30.666693
Train Epoch: 102 [1100/1248 (88%)] Loss: 28.396328
Train Epoch: 102 [1150/1248 (92%)] Loss: 29.191690
Train Epoch: 102 [1200/1248 (96%)] Loss: 28.500006
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.81it/s]
    epoch          : 102
    loss           : 31.12757255554199
    grad_norm      : 3806.607694091797
    val_loss       : 27.775661125457543
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch102.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:34<00:00,  3.73it/s]
Train Epoch: 103 [0/1248 (0%)] Loss: 34.616814
Train Epoch: 103 [50/1248 (4%)] Loss: 31.013288
Train Epoch: 103 [100/1248 (8%)] Loss: 33.519527
Train Epoch: 103 [150/1248 (12%)] Loss: 32.489895
Train Epoch: 103 [200/1248 (16%)] Loss: 32.560566
Train Epoch: 103 [250/1248 (20%)] Loss: 32.493778
Train Epoch: 103 [300/1248 (24%)] Loss: 30.896566
Train Epoch: 103 [350/1248 (28%)] Loss: 30.625216
Train Epoch: 103 [400/1248 (32%)] Loss: 28.526697
Train Epoch: 103 [450/1248 (36%)] Loss: 29.972105
Train Epoch: 103 [500/1248 (40%)] Loss: 30.663918
Train Epoch: 103 [550/1248 (44%)] Loss: 28.564035
Train Epoch: 103 [600/1248 (48%)] Loss: 31.754143
Train Epoch: 103 [650/1248 (52%)] Loss: 31.837589
Train Epoch: 103 [700/1248 (56%)] Loss: 32.394165
Train Epoch: 103 [750/1248 (60%)] Loss: 34.989361
Train Epoch: 103 [800/1248 (64%)] Loss: 30.590900
Train Epoch: 103 [850/1248 (68%)] Loss: 30.991873
Train Epoch: 103 [900/1248 (72%)] Loss: 28.560299
Train Epoch: 103 [950/1248 (76%)] Loss: 33.666512
Train Epoch: 103 [1000/1248 (80%)] Loss: 31.684055
Train Epoch: 103 [1050/1248 (84%)] Loss: 31.770357
Train Epoch: 103 [1100/1248 (88%)] Loss: 31.269201
Train Epoch: 103 [1150/1248 (92%)] Loss: 31.576490
Train Epoch: 103 [1200/1248 (96%)] Loss: 32.579620
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:24<00:00,  5.73it/s]
    epoch          : 103
    loss           : 31.65226909637451
    grad_norm      : 3548.5559692382812
    val_loss       : 28.060584761255935
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch103.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:33<00:00,  3.74it/s]
Train Epoch: 104 [0/1248 (0%)] Loss: 30.225677
Train Epoch: 104 [50/1248 (4%)] Loss: 29.010216
Train Epoch: 104 [100/1248 (8%)] Loss: 31.917198
Train Epoch: 104 [150/1248 (12%)] Loss: 30.892567
Train Epoch: 104 [200/1248 (16%)] Loss: 30.333054
Train Epoch: 104 [250/1248 (20%)] Loss: 30.080807
Train Epoch: 104 [300/1248 (24%)] Loss: 29.750877
Train Epoch: 104 [350/1248 (28%)] Loss: 26.930727
Train Epoch: 104 [400/1248 (32%)] Loss: 30.699556
Train Epoch: 104 [450/1248 (36%)] Loss: 29.556667
Train Epoch: 104 [500/1248 (40%)] Loss: 32.055805
Train Epoch: 104 [550/1248 (44%)] Loss: 31.032177
Train Epoch: 104 [600/1248 (48%)] Loss: 31.776203
Train Epoch: 104 [650/1248 (52%)] Loss: 29.571743
Train Epoch: 104 [700/1248 (56%)] Loss: 27.551533
Train Epoch: 104 [750/1248 (60%)] Loss: 28.437363
Train Epoch: 104 [800/1248 (64%)] Loss: 31.610039
Train Epoch: 104 [850/1248 (68%)] Loss: 30.622829
Train Epoch: 104 [900/1248 (72%)] Loss: 29.750160
Train Epoch: 104 [950/1248 (76%)] Loss: 32.274464
Train Epoch: 104 [1000/1248 (80%)] Loss: 31.263916
Train Epoch: 104 [1050/1248 (84%)] Loss: 32.947277
Train Epoch: 104 [1100/1248 (88%)] Loss: 30.424252
Train Epoch: 104 [1150/1248 (92%)] Loss: 28.915211
Train Epoch: 104 [1200/1248 (96%)] Loss: 31.830690
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.92it/s]
    epoch          : 104
    loss           : 31.60818130493164
    grad_norm      : 3851.732825317383
    val_loss       : 27.96554820657634
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch104.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:38<00:00,  3.68it/s]
Train Epoch: 105 [0/1248 (0%)] Loss: 29.293850
Train Epoch: 105 [50/1248 (4%)] Loss: 31.929834
Train Epoch: 105 [100/1248 (8%)] Loss: 33.020172
Train Epoch: 105 [150/1248 (12%)] Loss: 32.006371
Train Epoch: 105 [200/1248 (16%)] Loss: 29.818117
Train Epoch: 105 [250/1248 (20%)] Loss: 32.291035
Train Epoch: 105 [300/1248 (24%)] Loss: 31.970783
Train Epoch: 105 [350/1248 (28%)] Loss: 28.678339
Train Epoch: 105 [400/1248 (32%)] Loss: 32.016270
Train Epoch: 105 [450/1248 (36%)] Loss: 33.613400
Train Epoch: 105 [500/1248 (40%)] Loss: 27.828863
Train Epoch: 105 [550/1248 (44%)] Loss: 31.687164
Train Epoch: 105 [600/1248 (48%)] Loss: 32.459209
Train Epoch: 105 [650/1248 (52%)] Loss: 32.521721
Train Epoch: 105 [700/1248 (56%)] Loss: 32.627892
Train Epoch: 105 [750/1248 (60%)] Loss: 33.992977
Train Epoch: 105 [800/1248 (64%)] Loss: 33.645996
Train Epoch: 105 [850/1248 (68%)] Loss: 31.529425
Train Epoch: 105 [900/1248 (72%)] Loss: 32.832207
Train Epoch: 105 [950/1248 (76%)] Loss: 30.304058
Train Epoch: 105 [1000/1248 (80%)] Loss: 34.170982
Train Epoch: 105 [1050/1248 (84%)] Loss: 34.279583
Train Epoch: 105 [1100/1248 (88%)] Loss: 29.512146
Train Epoch: 105 [1150/1248 (92%)] Loss: 31.830275
Train Epoch: 105 [1200/1248 (96%)] Loss: 29.934160
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.82it/s]
    epoch          : 105
    loss           : 31.28743064880371
    grad_norm      : 4232.2427734375
    val_loss       : 28.20460161716818
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch105.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:36<00:00,  3.71it/s]
Train Epoch: 106 [0/1248 (0%)] Loss: 32.895142
Train Epoch: 106 [50/1248 (4%)] Loss: 30.519091
Train Epoch: 106 [100/1248 (8%)] Loss: 29.604504
Train Epoch: 106 [150/1248 (12%)] Loss: 30.823549
Train Epoch: 106 [200/1248 (16%)] Loss: 32.056404
Train Epoch: 106 [250/1248 (20%)] Loss: 31.026369
Train Epoch: 106 [300/1248 (24%)] Loss: 31.124496
Train Epoch: 106 [350/1248 (28%)] Loss: 30.896847
Train Epoch: 106 [400/1248 (32%)] Loss: 33.616917
Train Epoch: 106 [450/1248 (36%)] Loss: 29.562517
Train Epoch: 106 [500/1248 (40%)] Loss: 31.341862
Train Epoch: 106 [550/1248 (44%)] Loss: 32.192951
Train Epoch: 106 [600/1248 (48%)] Loss: 30.412590
Train Epoch: 106 [650/1248 (52%)] Loss: 31.103149
Train Epoch: 106 [700/1248 (56%)] Loss: 29.260403
Train Epoch: 106 [750/1248 (60%)] Loss: 30.583515
Train Epoch: 106 [800/1248 (64%)] Loss: 33.695606
Train Epoch: 106 [850/1248 (68%)] Loss: 30.827158
Train Epoch: 106 [900/1248 (72%)] Loss: 31.466726
Train Epoch: 106 [950/1248 (76%)] Loss: 30.840742
Train Epoch: 106 [1000/1248 (80%)] Loss: 33.182522
Train Epoch: 106 [1050/1248 (84%)] Loss: 30.523100
Train Epoch: 106 [1100/1248 (88%)] Loss: 31.341366
Train Epoch: 106 [1150/1248 (92%)] Loss: 31.859573
Train Epoch: 106 [1200/1248 (96%)] Loss: 26.413437
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:24<00:00,  5.72it/s]
    epoch          : 106
    loss           : 31.641759910583495
    grad_norm      : 3259.8281494140624
    val_loss       : 27.59132499145947
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch106.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:31<00:00,  3.76it/s]
Train Epoch: 107 [0/1248 (0%)] Loss: 30.195654
Train Epoch: 107 [50/1248 (4%)] Loss: 31.106340
Train Epoch: 107 [100/1248 (8%)] Loss: 31.397171
Train Epoch: 107 [150/1248 (12%)] Loss: 29.241737
Train Epoch: 107 [200/1248 (16%)] Loss: 33.284649
Train Epoch: 107 [250/1248 (20%)] Loss: 31.471975
Train Epoch: 107 [300/1248 (24%)] Loss: 31.772554
Train Epoch: 107 [350/1248 (28%)] Loss: 29.165129
Train Epoch: 107 [400/1248 (32%)] Loss: 31.919205
Train Epoch: 107 [450/1248 (36%)] Loss: 33.176811
Train Epoch: 107 [500/1248 (40%)] Loss: 31.997570
Train Epoch: 107 [550/1248 (44%)] Loss: 32.245594
Train Epoch: 107 [600/1248 (48%)] Loss: 33.397984
Train Epoch: 107 [650/1248 (52%)] Loss: 33.124660
Train Epoch: 107 [700/1248 (56%)] Loss: 30.862906
Train Epoch: 107 [750/1248 (60%)] Loss: 36.084461
Train Epoch: 107 [800/1248 (64%)] Loss: 31.153196
Train Epoch: 107 [850/1248 (68%)] Loss: 33.217243
Train Epoch: 107 [900/1248 (72%)] Loss: 31.443499
Train Epoch: 107 [950/1248 (76%)] Loss: 30.604074
Train Epoch: 107 [1000/1248 (80%)] Loss: 30.610573
Train Epoch: 107 [1050/1248 (84%)] Loss: 29.217958
Train Epoch: 107 [1100/1248 (88%)] Loss: 30.067268
Train Epoch: 107 [1150/1248 (92%)] Loss: 32.127628
Train Epoch: 107 [1200/1248 (96%)] Loss: 31.517319
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  6.02it/s]
    epoch          : 107
    loss           : 31.48026496887207
    grad_norm      : 3504.4263623046877
    val_loss       : 27.871225206114406
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch107.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:31<00:00,  3.76it/s]
Train Epoch: 108 [0/1248 (0%)] Loss: 28.319712
Train Epoch: 108 [50/1248 (4%)] Loss: 29.808683
Train Epoch: 108 [100/1248 (8%)] Loss: 28.273355
Train Epoch: 108 [150/1248 (12%)] Loss: 31.852468
Train Epoch: 108 [200/1248 (16%)] Loss: 32.858868
Train Epoch: 108 [250/1248 (20%)] Loss: 32.195213
Train Epoch: 108 [300/1248 (24%)] Loss: 32.472786
Train Epoch: 108 [350/1248 (28%)] Loss: 32.997913
Train Epoch: 108 [400/1248 (32%)] Loss: 32.634640
Train Epoch: 108 [450/1248 (36%)] Loss: 27.266861
Train Epoch: 108 [500/1248 (40%)] Loss: 33.066643
Train Epoch: 108 [550/1248 (44%)] Loss: 29.275280
Train Epoch: 108 [600/1248 (48%)] Loss: 29.176247
Train Epoch: 108 [650/1248 (52%)] Loss: 33.353519
Train Epoch: 108 [700/1248 (56%)] Loss: 31.005810
Train Epoch: 108 [750/1248 (60%)] Loss: 29.829195
Train Epoch: 108 [800/1248 (64%)] Loss: 33.114407
Train Epoch: 108 [850/1248 (68%)] Loss: 30.044170
Train Epoch: 108 [900/1248 (72%)] Loss: 30.837547
Train Epoch: 108 [950/1248 (76%)] Loss: 30.199297
Train Epoch: 108 [1000/1248 (80%)] Loss: 30.424183
Train Epoch: 108 [1050/1248 (84%)] Loss: 34.026165
Train Epoch: 108 [1100/1248 (88%)] Loss: 31.708204
Train Epoch: 108 [1150/1248 (92%)] Loss: 33.627079
Train Epoch: 108 [1200/1248 (96%)] Loss: 32.099167
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:24<00:00,  5.71it/s]
    epoch          : 108
    loss           : 31.403966369628908
    grad_norm      : 3580.9859838867187
    val_loss       : 28.264728107040735
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch108.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:40<00:00,  3.67it/s]
Train Epoch: 109 [0/1248 (0%)] Loss: 31.217953
Train Epoch: 109 [50/1248 (4%)] Loss: 29.824192
Train Epoch: 109 [100/1248 (8%)] Loss: 32.202511
Train Epoch: 109 [150/1248 (12%)] Loss: 31.829523
Train Epoch: 109 [200/1248 (16%)] Loss: 29.188679
Train Epoch: 109 [250/1248 (20%)] Loss: 27.934826
Train Epoch: 109 [300/1248 (24%)] Loss: 29.485237
Train Epoch: 109 [350/1248 (28%)] Loss: 33.639523
Train Epoch: 109 [400/1248 (32%)] Loss: 32.510334
Train Epoch: 109 [450/1248 (36%)] Loss: 29.184465
Train Epoch: 109 [500/1248 (40%)] Loss: 29.889286
Train Epoch: 109 [550/1248 (44%)] Loss: 31.103315
Train Epoch: 109 [600/1248 (48%)] Loss: 33.522453
Train Epoch: 109 [650/1248 (52%)] Loss: 34.534733
Train Epoch: 109 [700/1248 (56%)] Loss: 33.906006
Train Epoch: 109 [750/1248 (60%)] Loss: 32.609592
Train Epoch: 109 [800/1248 (64%)] Loss: 31.009792
Train Epoch: 109 [850/1248 (68%)] Loss: 31.561733
Train Epoch: 109 [900/1248 (72%)] Loss: 32.225212
Train Epoch: 109 [950/1248 (76%)] Loss: 33.325779
Train Epoch: 109 [1000/1248 (80%)] Loss: 34.406723
Train Epoch: 109 [1050/1248 (84%)] Loss: 32.709507
Train Epoch: 109 [1100/1248 (88%)] Loss: 33.380051
Train Epoch: 109 [1150/1248 (92%)] Loss: 31.314425
Train Epoch: 109 [1200/1248 (96%)] Loss: 29.827951
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:25<00:00,  5.46it/s]
    epoch          : 109
    loss           : 31.25759792327881
    grad_norm      : 3963.315427246094
    val_loss       : 29.133572146189298
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch109.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:44<00:00,  3.62it/s]
Train Epoch: 110 [0/1248 (0%)] Loss: 33.025143
Train Epoch: 110 [50/1248 (4%)] Loss: 31.007427
Train Epoch: 110 [100/1248 (8%)] Loss: 29.971981
Train Epoch: 110 [150/1248 (12%)] Loss: 34.553673
Train Epoch: 110 [200/1248 (16%)] Loss: 34.593433
Train Epoch: 110 [250/1248 (20%)] Loss: 28.840446
Train Epoch: 110 [300/1248 (24%)] Loss: 29.944525
Train Epoch: 110 [350/1248 (28%)] Loss: 32.551464
Train Epoch: 110 [400/1248 (32%)] Loss: 30.351162
Train Epoch: 110 [450/1248 (36%)] Loss: 31.694702
Train Epoch: 110 [500/1248 (40%)] Loss: 30.772268
Train Epoch: 110 [550/1248 (44%)] Loss: 31.563063
Train Epoch: 110 [600/1248 (48%)] Loss: 30.893509
Train Epoch: 110 [650/1248 (52%)] Loss: 32.338116
Train Epoch: 110 [700/1248 (56%)] Loss: 29.210289
Train Epoch: 110 [750/1248 (60%)] Loss: 33.396355
Train Epoch: 110 [800/1248 (64%)] Loss: 32.299187
Train Epoch: 110 [850/1248 (68%)] Loss: 30.346024
Train Epoch: 110 [900/1248 (72%)] Loss: 33.848244
Train Epoch: 110 [950/1248 (76%)] Loss: 31.340080
Train Epoch: 110 [1000/1248 (80%)] Loss: 31.091242
Train Epoch: 110 [1050/1248 (84%)] Loss: 27.078392
Train Epoch: 110 [1100/1248 (88%)] Loss: 34.445656
Train Epoch: 110 [1150/1248 (92%)] Loss: 28.400232
Train Epoch: 110 [1200/1248 (96%)] Loss: 33.764565
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:24<00:00,  5.63it/s]
    epoch          : 110
    loss           : 31.65061958312988
    grad_norm      : 3650.4953857421874
    val_loss       : 27.605855598724144
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch110.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:46<00:00,  3.60it/s]
Train Epoch: 111 [0/1248 (0%)] Loss: 29.664196
Train Epoch: 111 [50/1248 (4%)] Loss: 31.657337
Train Epoch: 111 [100/1248 (8%)] Loss: 29.592686
Train Epoch: 111 [150/1248 (12%)] Loss: 33.740150
Train Epoch: 111 [200/1248 (16%)] Loss: 33.911095
Train Epoch: 111 [250/1248 (20%)] Loss: 33.004417
Train Epoch: 111 [300/1248 (24%)] Loss: 30.994215
Train Epoch: 111 [350/1248 (28%)] Loss: 34.229923
Train Epoch: 111 [400/1248 (32%)] Loss: 32.191891
Train Epoch: 111 [450/1248 (36%)] Loss: 32.277882
Train Epoch: 111 [500/1248 (40%)] Loss: 32.905685
Train Epoch: 111 [550/1248 (44%)] Loss: 30.280762
Train Epoch: 111 [600/1248 (48%)] Loss: 34.812107
Train Epoch: 111 [650/1248 (52%)] Loss: 29.132450
Train Epoch: 111 [700/1248 (56%)] Loss: 30.492510
Train Epoch: 111 [750/1248 (60%)] Loss: 31.080105
Train Epoch: 111 [800/1248 (64%)] Loss: 33.775871
Train Epoch: 111 [850/1248 (68%)] Loss: 31.468388
Train Epoch: 111 [900/1248 (72%)] Loss: 29.304396
Train Epoch: 111 [950/1248 (76%)] Loss: 33.153847
Train Epoch: 111 [1000/1248 (80%)] Loss: 27.543861
Train Epoch: 111 [1050/1248 (84%)] Loss: 32.269859
Train Epoch: 111 [1100/1248 (88%)] Loss: 31.408047
Train Epoch: 111 [1150/1248 (92%)] Loss: 31.269133
Train Epoch: 111 [1200/1248 (96%)] Loss: 32.151794
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:26<00:00,  5.30it/s]
    epoch          : 111
    loss           : 31.38594268798828
    grad_norm      : 4053.8339501953124
    val_loss       : 28.602352869596412
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch111.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:44<00:00,  3.62it/s]
Train Epoch: 112 [0/1248 (0%)] Loss: 30.635685
Train Epoch: 112 [50/1248 (4%)] Loss: 32.325424
Train Epoch: 112 [100/1248 (8%)] Loss: 31.155636
Train Epoch: 112 [150/1248 (12%)] Loss: 31.934038
Train Epoch: 112 [200/1248 (16%)] Loss: 30.402788
Train Epoch: 112 [250/1248 (20%)] Loss: 32.886211
Train Epoch: 112 [300/1248 (24%)] Loss: 28.944824
Train Epoch: 112 [350/1248 (28%)] Loss: 29.709578
Train Epoch: 112 [400/1248 (32%)] Loss: 31.098742
Train Epoch: 112 [450/1248 (36%)] Loss: 32.039299
Train Epoch: 112 [500/1248 (40%)] Loss: 34.109573
Train Epoch: 112 [550/1248 (44%)] Loss: 31.339577
Train Epoch: 112 [600/1248 (48%)] Loss: 30.719515
Train Epoch: 112 [650/1248 (52%)] Loss: 30.219269
Train Epoch: 112 [700/1248 (56%)] Loss: 30.194023
Train Epoch: 112 [750/1248 (60%)] Loss: 34.334984
Train Epoch: 112 [800/1248 (64%)] Loss: 28.401554
Train Epoch: 112 [850/1248 (68%)] Loss: 31.461575
Train Epoch: 112 [900/1248 (72%)] Loss: 29.532078
Train Epoch: 112 [950/1248 (76%)] Loss: 30.531073
Train Epoch: 112 [1000/1248 (80%)] Loss: 29.784582
Train Epoch: 112 [1050/1248 (84%)] Loss: 34.197262
Train Epoch: 112 [1100/1248 (88%)] Loss: 29.936785
Train Epoch: 112 [1150/1248 (92%)] Loss: 32.379696
Train Epoch: 112 [1200/1248 (96%)] Loss: 30.542204
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:24<00:00,  5.64it/s]
    epoch          : 112
    loss           : 31.501470375061036
    grad_norm      : 3916.7750659179687
    val_loss       : 27.721831163914082
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch112.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:37<00:00,  3.70it/s]
Train Epoch: 113 [0/1248 (0%)] Loss: 33.262711
Train Epoch: 113 [50/1248 (4%)] Loss: 30.264458
Train Epoch: 113 [100/1248 (8%)] Loss: 31.467100
Train Epoch: 113 [150/1248 (12%)] Loss: 28.093676
Train Epoch: 113 [200/1248 (16%)] Loss: 32.486599
Train Epoch: 113 [250/1248 (20%)] Loss: 32.894028
Train Epoch: 113 [300/1248 (24%)] Loss: 32.164818
Train Epoch: 113 [350/1248 (28%)] Loss: 32.558266
Train Epoch: 113 [400/1248 (32%)] Loss: 32.438789
Train Epoch: 113 [450/1248 (36%)] Loss: 34.885048
Train Epoch: 113 [500/1248 (40%)] Loss: 31.005159
Train Epoch: 113 [550/1248 (44%)] Loss: 30.953526
Train Epoch: 113 [600/1248 (48%)] Loss: 31.707262
Train Epoch: 113 [650/1248 (52%)] Loss: 31.985968
Train Epoch: 113 [700/1248 (56%)] Loss: 33.356186
Train Epoch: 113 [750/1248 (60%)] Loss: 31.419861
Train Epoch: 113 [800/1248 (64%)] Loss: 35.211452
Train Epoch: 113 [850/1248 (68%)] Loss: 29.567451
Train Epoch: 113 [900/1248 (72%)] Loss: 31.279577
Train Epoch: 113 [950/1248 (76%)] Loss: 32.133045
Train Epoch: 113 [1000/1248 (80%)] Loss: 28.940235
Train Epoch: 113 [1050/1248 (84%)] Loss: 32.181396
Train Epoch: 113 [1100/1248 (88%)] Loss: 30.155985
Train Epoch: 113 [1150/1248 (92%)] Loss: 34.992001
Train Epoch: 113 [1200/1248 (96%)] Loss: 32.525246
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:24<00:00,  5.58it/s]
    epoch          : 113
    loss           : 31.214320907592775
    grad_norm      : 3918.5154296875
    val_loss       : 28.335344410628725
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch113.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:36<00:00,  3.70it/s]
Train Epoch: 114 [0/1248 (0%)] Loss: 29.935867
Train Epoch: 114 [50/1248 (4%)] Loss: 30.707977
Train Epoch: 114 [100/1248 (8%)] Loss: 31.550289
Train Epoch: 114 [150/1248 (12%)] Loss: 29.432859
Train Epoch: 114 [200/1248 (16%)] Loss: 32.597557
Train Epoch: 114 [250/1248 (20%)] Loss: 33.105221
Train Epoch: 114 [300/1248 (24%)] Loss: 28.619493
Train Epoch: 114 [350/1248 (28%)] Loss: 34.484859
Train Epoch: 114 [400/1248 (32%)] Loss: 30.976370
Train Epoch: 114 [450/1248 (36%)] Loss: 35.492821
Train Epoch: 114 [500/1248 (40%)] Loss: 32.661346
Train Epoch: 114 [550/1248 (44%)] Loss: 29.488380
Train Epoch: 114 [600/1248 (48%)] Loss: 34.564152
Train Epoch: 114 [650/1248 (52%)] Loss: 34.043221
Train Epoch: 114 [700/1248 (56%)] Loss: 33.715885
Train Epoch: 114 [750/1248 (60%)] Loss: 30.414059
Train Epoch: 114 [800/1248 (64%)] Loss: 31.437748
Train Epoch: 114 [850/1248 (68%)] Loss: 29.926113
Train Epoch: 114 [900/1248 (72%)] Loss: 32.113609
Train Epoch: 114 [950/1248 (76%)] Loss: 28.730801
Train Epoch: 114 [1000/1248 (80%)] Loss: 29.421247
Train Epoch: 114 [1050/1248 (84%)] Loss: 31.041460
Train Epoch: 114 [1100/1248 (88%)] Loss: 30.536369
Train Epoch: 114 [1150/1248 (92%)] Loss: 30.025255
Train Epoch: 114 [1200/1248 (96%)] Loss: 31.593159
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.81it/s]
    epoch          : 114
    loss           : 31.700830307006836
    grad_norm      : 3877.035549316406
    val_loss       : 27.14045335234498
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch114.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:34<00:00,  3.73it/s]
Train Epoch: 115 [0/1248 (0%)] Loss: 30.838451
Train Epoch: 115 [50/1248 (4%)] Loss: 30.825539
Train Epoch: 115 [100/1248 (8%)] Loss: 30.113739
Train Epoch: 115 [150/1248 (12%)] Loss: 31.406574
Train Epoch: 115 [200/1248 (16%)] Loss: 29.229843
Train Epoch: 115 [250/1248 (20%)] Loss: 33.423229
Train Epoch: 115 [300/1248 (24%)] Loss: 33.746555
Train Epoch: 115 [350/1248 (28%)] Loss: 26.583160
Train Epoch: 115 [400/1248 (32%)] Loss: 29.456785
Train Epoch: 115 [450/1248 (36%)] Loss: 26.933266
Train Epoch: 115 [500/1248 (40%)] Loss: 32.023327
Train Epoch: 115 [550/1248 (44%)] Loss: 32.889500
Train Epoch: 115 [600/1248 (48%)] Loss: 29.475178
Train Epoch: 115 [650/1248 (52%)] Loss: 31.916986
Train Epoch: 115 [700/1248 (56%)] Loss: 34.875751
Train Epoch: 115 [750/1248 (60%)] Loss: 31.507778
Train Epoch: 115 [800/1248 (64%)] Loss: 32.620693
Train Epoch: 115 [850/1248 (68%)] Loss: 36.283257
Train Epoch: 115 [900/1248 (72%)] Loss: 31.761288
Train Epoch: 115 [950/1248 (76%)] Loss: 34.378437
Train Epoch: 115 [1000/1248 (80%)] Loss: 32.160660
Train Epoch: 115 [1050/1248 (84%)] Loss: 32.927021
Train Epoch: 115 [1100/1248 (88%)] Loss: 34.552742
Train Epoch: 115 [1150/1248 (92%)] Loss: 30.338423
Train Epoch: 115 [1200/1248 (96%)] Loss: 30.546736
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.99it/s]
    epoch          : 115
    loss           : 30.905501899719237
    grad_norm      : 4264.5228515625
    val_loss       : 27.79588430390941
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch115.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:30<00:00,  3.77it/s]
Train Epoch: 116 [0/1248 (0%)] Loss: 33.459454
Train Epoch: 116 [50/1248 (4%)] Loss: 33.922508
Train Epoch: 116 [100/1248 (8%)] Loss: 33.690010
Train Epoch: 116 [150/1248 (12%)] Loss: 30.521749
Train Epoch: 116 [200/1248 (16%)] Loss: 32.259331
Train Epoch: 116 [250/1248 (20%)] Loss: 29.914265
Train Epoch: 116 [300/1248 (24%)] Loss: 35.472565
Train Epoch: 116 [350/1248 (28%)] Loss: 28.915890
Train Epoch: 116 [400/1248 (32%)] Loss: 31.744791
Train Epoch: 116 [450/1248 (36%)] Loss: 27.487921
Train Epoch: 116 [500/1248 (40%)] Loss: 32.904335
Train Epoch: 116 [550/1248 (44%)] Loss: 31.312162
Train Epoch: 116 [600/1248 (48%)] Loss: 33.628647
Train Epoch: 116 [650/1248 (52%)] Loss: 28.778101
Train Epoch: 116 [700/1248 (56%)] Loss: 31.388817
Train Epoch: 116 [750/1248 (60%)] Loss: 29.465191
Train Epoch: 116 [800/1248 (64%)] Loss: 29.986326
Train Epoch: 116 [850/1248 (68%)] Loss: 29.936298
Train Epoch: 116 [900/1248 (72%)] Loss: 34.750835
Train Epoch: 116 [950/1248 (76%)] Loss: 30.839420
Train Epoch: 116 [1000/1248 (80%)] Loss: 30.367197
Train Epoch: 116 [1050/1248 (84%)] Loss: 32.830158
Train Epoch: 116 [1100/1248 (88%)] Loss: 32.927784
Train Epoch: 116 [1150/1248 (92%)] Loss: 33.679966
Train Epoch: 116 [1200/1248 (96%)] Loss: 33.858852
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.94it/s]
    epoch          : 116
    loss           : 31.578541984558104
    grad_norm      : 4220.726008300781
    val_loss       : 28.65134414837515
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch116.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:40<00:00,  3.66it/s]
Train Epoch: 117 [0/1248 (0%)] Loss: 32.655933
Train Epoch: 117 [50/1248 (4%)] Loss: 31.798979
Train Epoch: 117 [100/1248 (8%)] Loss: 29.509718
Train Epoch: 117 [150/1248 (12%)] Loss: 30.932400
Train Epoch: 117 [200/1248 (16%)] Loss: 32.860298
Train Epoch: 117 [250/1248 (20%)] Loss: 33.810562
Train Epoch: 117 [300/1248 (24%)] Loss: 32.437984
Train Epoch: 117 [350/1248 (28%)] Loss: 30.815071
Train Epoch: 117 [400/1248 (32%)] Loss: 30.392593
Train Epoch: 117 [450/1248 (36%)] Loss: 29.842173
Train Epoch: 117 [500/1248 (40%)] Loss: 28.458788
Train Epoch: 117 [550/1248 (44%)] Loss: 30.285553
Train Epoch: 117 [600/1248 (48%)] Loss: 30.788404
Train Epoch: 117 [650/1248 (52%)] Loss: 29.943262
Train Epoch: 117 [700/1248 (56%)] Loss: 30.616930
Train Epoch: 117 [750/1248 (60%)] Loss: 31.540852
Train Epoch: 117 [800/1248 (64%)] Loss: 32.057846
Train Epoch: 117 [850/1248 (68%)] Loss: 32.533318
Train Epoch: 117 [900/1248 (72%)] Loss: 29.272865
Train Epoch: 117 [950/1248 (76%)] Loss: 31.444351
Train Epoch: 117 [1000/1248 (80%)] Loss: 31.862009
Train Epoch: 117 [1050/1248 (84%)] Loss: 31.970881
Train Epoch: 117 [1100/1248 (88%)] Loss: 28.920685
Train Epoch: 117 [1150/1248 (92%)] Loss: 30.582342
Train Epoch: 117 [1200/1248 (96%)] Loss: 30.516058
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:24<00:00,  5.58it/s]
    epoch          : 117
    loss           : 31.57658805847168
    grad_norm      : 3642.7776538085936
    val_loss       : 29.36896844218961
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch117.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:41<00:00,  3.66it/s]
Train Epoch: 118 [0/1248 (0%)] Loss: 34.148582
Train Epoch: 118 [50/1248 (4%)] Loss: 31.728811
Train Epoch: 118 [100/1248 (8%)] Loss: 31.757874
Train Epoch: 118 [150/1248 (12%)] Loss: 31.716303
Train Epoch: 118 [200/1248 (16%)] Loss: 31.993938
Train Epoch: 118 [250/1248 (20%)] Loss: 30.042583
Train Epoch: 118 [300/1248 (24%)] Loss: 30.574457
Train Epoch: 118 [350/1248 (28%)] Loss: 33.659508
Train Epoch: 118 [400/1248 (32%)] Loss: 32.891842
Train Epoch: 118 [450/1248 (36%)] Loss: 32.182270
Train Epoch: 118 [500/1248 (40%)] Loss: 32.511166
Train Epoch: 118 [550/1248 (44%)] Loss: 33.191685
Train Epoch: 118 [600/1248 (48%)] Loss: 31.288137
Train Epoch: 118 [650/1248 (52%)] Loss: 30.799349
Train Epoch: 118 [700/1248 (56%)] Loss: 32.856277
Train Epoch: 118 [750/1248 (60%)] Loss: 30.911026
Train Epoch: 118 [800/1248 (64%)] Loss: 33.443844
Train Epoch: 118 [850/1248 (68%)] Loss: 31.927277
Train Epoch: 118 [900/1248 (72%)] Loss: 32.974983
Train Epoch: 118 [950/1248 (76%)] Loss: 33.362923
Train Epoch: 118 [1000/1248 (80%)] Loss: 31.228701
Train Epoch: 118 [1050/1248 (84%)] Loss: 30.343594
Train Epoch: 118 [1100/1248 (88%)] Loss: 29.038704
Train Epoch: 118 [1150/1248 (92%)] Loss: 31.242393
Train Epoch: 118 [1200/1248 (96%)] Loss: 31.122349
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:25<00:00,  5.56it/s]
    epoch          : 118
    loss           : 31.583997268676757
    grad_norm      : 4247.014631347656
    val_loss       : 29.289303827628814
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch118.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:39<00:00,  3.67it/s]
Train Epoch: 119 [0/1248 (0%)] Loss: 33.646484
Train Epoch: 119 [50/1248 (4%)] Loss: 31.304335
Train Epoch: 119 [100/1248 (8%)] Loss: 31.299946
Train Epoch: 119 [150/1248 (12%)] Loss: 32.883385
Train Epoch: 119 [200/1248 (16%)] Loss: 31.607018
Train Epoch: 119 [250/1248 (20%)] Loss: 32.021179
Train Epoch: 119 [300/1248 (24%)] Loss: 31.237806
Train Epoch: 119 [350/1248 (28%)] Loss: 32.901173
Train Epoch: 119 [400/1248 (32%)] Loss: 32.172771
Train Epoch: 119 [450/1248 (36%)] Loss: 29.964035
Train Epoch: 119 [500/1248 (40%)] Loss: 30.406105
Train Epoch: 119 [550/1248 (44%)] Loss: 31.356955
Train Epoch: 119 [600/1248 (48%)] Loss: 30.395142
Train Epoch: 119 [650/1248 (52%)] Loss: 30.710808
Train Epoch: 119 [700/1248 (56%)] Loss: 29.713785
Train Epoch: 119 [750/1248 (60%)] Loss: 34.101189
Train Epoch: 119 [800/1248 (64%)] Loss: 31.589716
Train Epoch: 119 [850/1248 (68%)] Loss: 31.480238
Train Epoch: 119 [900/1248 (72%)] Loss: 32.276981
Train Epoch: 119 [950/1248 (76%)] Loss: 30.666767
Train Epoch: 119 [1000/1248 (80%)] Loss: 31.228270
Train Epoch: 119 [1050/1248 (84%)] Loss: 29.628082
Train Epoch: 119 [1100/1248 (88%)] Loss: 31.759207
Train Epoch: 119 [1150/1248 (92%)] Loss: 33.627186
Train Epoch: 119 [1200/1248 (96%)] Loss: 33.168259
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.91it/s]
    epoch          : 119
    loss           : 31.526513633728026
    grad_norm      : 3960.929530029297
    val_loss       : 28.426234924535958
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch119.pth
train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1247/1248 [05:37<00:00,  3.69it/s]
Train Epoch: 120 [0/1248 (0%)] Loss: 31.010561
Train Epoch: 120 [50/1248 (4%)] Loss: 31.239014
Train Epoch: 120 [100/1248 (8%)] Loss: 29.945564
Train Epoch: 120 [150/1248 (12%)] Loss: 33.593483
Train Epoch: 120 [200/1248 (16%)] Loss: 31.055508
Train Epoch: 120 [250/1248 (20%)] Loss: 30.635937
Train Epoch: 120 [300/1248 (24%)] Loss: 35.408882
Train Epoch: 120 [350/1248 (28%)] Loss: 30.809233
Train Epoch: 120 [400/1248 (32%)] Loss: 36.764053
Train Epoch: 120 [450/1248 (36%)] Loss: 32.370533
Train Epoch: 120 [500/1248 (40%)] Loss: 31.197863
Train Epoch: 120 [550/1248 (44%)] Loss: 30.444410
Train Epoch: 120 [600/1248 (48%)] Loss: 31.229078
Train Epoch: 120 [650/1248 (52%)] Loss: 34.193489
Train Epoch: 120 [700/1248 (56%)] Loss: 30.624121
Train Epoch: 120 [750/1248 (60%)] Loss: 31.219976
Train Epoch: 120 [800/1248 (64%)] Loss: 31.509811
Train Epoch: 120 [850/1248 (68%)] Loss: 28.903000
Train Epoch: 120 [900/1248 (72%)] Loss: 33.773972
Train Epoch: 120 [950/1248 (76%)] Loss: 32.953709
Train Epoch: 120 [1000/1248 (80%)] Loss: 31.378035
Train Epoch: 120 [1050/1248 (84%)] Loss: 33.056931
Train Epoch: 120 [1100/1248 (88%)] Loss: 32.293011
Train Epoch: 120 [1150/1248 (92%)] Loss: 33.632923
Train Epoch: 120 [1200/1248 (96%)] Loss: 29.659212
val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:23<00:00,  5.80it/s]
    epoch          : 120
    loss           : 31.299269180297852
    grad_norm      : 3414.5285461425783
    val_loss       : 27.736578083724428
Saving checkpoint: /workspace/pytorch_project_template-main/saved/hifigan_full_long/checkpoint-epoch120.pth
